{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bea35",
   "metadata": {},
   "source": [
    "# 1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e580452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = torch.nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.sigmoid(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c200d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.autoencoders = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.autoencoders.append(AutoEncoder(prev_dim, hidden_dim))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "    def forward(self, X, train_mode, **kwargs):\n",
    "        if train_mode == 'layerwise':\n",
    "            layer_number = kwargs.get('layer_number', None)\n",
    "            if layer_number is None or layer_number < 0 or layer_number >= len(self.autoencoders):\n",
    "                raise ValueError(\"Invalid layer number for layerwise training\")\n",
    "            encoded = torch.sigmoid(self.autoencoders[layer_number].encoder(X))\n",
    "            decoded = torch.sigmoid(self.autoencoders[layer_number].decoder(encoded))\n",
    "            return encoded, decoded\n",
    "            \n",
    "        elif train_mode == 'endtoend':\n",
    "            for autoencoder in self.autoencoders:\n",
    "                X = torch.sigmoid(autoencoder.encoder(X))\n",
    "            encoded = X\n",
    "            for autoencoder in reversed(self.autoencoders):\n",
    "                X = torch.sigmoid(autoencoder.decoder(X))\n",
    "            decoded = X\n",
    "            return encoded, decoded\n",
    "    \n",
    "    def train(self,\n",
    "              X,\n",
    "              compile,\n",
    "              train_mode,\n",
    "              iters,\n",
    "              optimizer,\n",
    "              rho=0.01,\n",
    "              beta=1.0,\n",
    "              batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = X.shape[0] - 1\n",
    "\n",
    "        if compile==\"True\":\n",
    "            train_model = torch.compile(self)\n",
    "        else:\n",
    "            train_model = self\n",
    "        \n",
    "        if train_mode == 'layerwise':\n",
    "            for layer_number in range(len(self.autoencoders)):\n",
    "                for _ in tqdm.tqdm(range(iters), desc=f\"Training layer {layer_number}\"):\n",
    "                    batch_idx = torch.randint(0, X.shape[0] - batch_size + 1, (1,)).item()\n",
    "                    X_batch = X[batch_idx : batch_idx + batch_size]\n",
    "                    optimizer.zero_grad()\n",
    "                    encoded, decoded = train_model(X_batch, train_mode='layerwise', layer_number=layer_number)\n",
    "                    loss_1 = torch.nn.functional.mse_loss(decoded, X_batch, reduction='sum')\n",
    "                    rho_hat = torch.mean(encoded, dim=0)\n",
    "                    loss_2 = torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))\n",
    "                    loss = loss_1 + beta * loss_2\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                X = torch.sigmoid(self.autoencoders[layer_number].encoder(X)).detach()\n",
    "        elif train_mode == 'endtoend':\n",
    "            for _ in tqdm.tqdm(range(iters)):\n",
    "                batch_idx = torch.randint(0, X.shape[0] - batch_size + 1, (1,)).item()\n",
    "                X_batch = X[batch_idx : batch_idx + batch_size]\n",
    "                optimizer.zero_grad()\n",
    "                encoded, decoded = train_model(X_batch, train_mode='endtoend')\n",
    "                loss_1 = torch.nn.functional.mse_loss(decoded, X_batch, reduction='sum')\n",
    "                rho_hat = torch.mean(encoded, dim=0)\n",
    "                loss_2 = torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))\n",
    "                loss = loss_1 + beta * loss_2\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode(self, X):\n",
    "        for autoencoder in self.autoencoders:\n",
    "            X = torch.sigmoid(autoencoder.encoder(X))\n",
    "        return X\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode_decode(self, X):\n",
    "        for autoencoder in self.autoencoders:\n",
    "            X = torch.sigmoid(autoencoder.encoder(X))\n",
    "        for autoencoder in reversed(self.autoencoders):\n",
    "            X = torch.sigmoid(autoencoder.decoder(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6df9f",
   "metadata": {},
   "source": [
    "## 1.1. Kickstart test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.load_digits(return_X_y=True, as_frame=False)\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NB_EPOCHS = 5000\n",
    "BATCH_SIZE = 256\n",
    "X = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "graph_encoder = GraphEncoder(input_dim=X.shape[1], hidden_dims=[32, 16, 8]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(graph_encoder.parameters(), lr=0.01)\n",
    "nb_iters = NB_EPOCHS * (X.shape[0] // BATCH_SIZE)\n",
    "graph_encoder.train(X, compile=\"compile\", train_mode=\"endtoend\", iters=nb_iters, optimizer=optimizer, rho=0.01, beta=1.0, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54636c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xh = graph_encoder.encode_decode(X).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 10, figsize=(15, 4))\n",
    "for column in range(10):\n",
    "    axes[0, column].imshow(X[column].to(\"cpu\").reshape(8, 8), cmap='gray')\n",
    "    axes[0, column].set_title(\"Original\")\n",
    "    axes[1, column].imshow(Xh[column].to(\"cpu\").reshape(8, 8), cmap='gray')\n",
    "    axes[1, column].set_title(\"enc-dec\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0bc1a",
   "metadata": {},
   "source": [
    "# 2. Test on paper benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699dd07",
   "metadata": {},
   "source": [
    "## 2.1. Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ea65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Wine\n",
    "X, Y_wine= sklearn.datasets.load_wine(return_X_y=True, as_frame=False)\n",
    "X = sklearn.preprocessing.MinMaxScaler().fit_transform(X)\n",
    "S_wine = sklearn.metrics.pairwise.cosine_similarity(X, X)\n",
    "NTS_wine = S_wine / np.sum(S_wine, axis=1, keepdims=True)\n",
    "print(\"NTS_wine.shape:\", NTS_wine.shape)\n",
    "print(\"Y_wine.shape:\", Y_wine.shape, \"with classes:\", np.unique(Y_wine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NB_EPOCHS = 5000\n",
    "BATCH_SIZE = 178\n",
    "X = torch.tensor(NTS_wine, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "graph_encoder = GraphEncoder(input_dim=X.shape[1], hidden_dims=[178, 128, 64]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(graph_encoder.parameters(), lr=0.01)\n",
    "nb_iters = NB_EPOCHS * (X.shape[0] // BATCH_SIZE)\n",
    "graph_encoder.train(X, compile=\"compile\", train_mode=\"layerwise\", iters=nb_iters, optimizer=optimizer, rho=0.01, beta=1.0, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = graph_encoder.encode(X).to('cpu')\n",
    "print(\"Latent shape:\", latent.shape)\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=len(set(Y_wine)), random_state=42)\n",
    "y_pred = kmeans.fit_predict(latent)\n",
    "score = sklearn.metrics.normalized_mutual_info_score(Y_wine, y_pred)\n",
    "print(f\"NMI: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63630c",
   "metadata": {},
   "source": [
    "## 2.2. 20-Newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86508a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "sklearn.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f219ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading 20 Newsgroups dataset...\")\n",
    "data_vectorized = sklearn.datasets.fetch_20newsgroups_vectorized(data_home=\"./20-newsgroup\", subset=\"all\")\n",
    "\n",
    "print(\"Converting to TF-IDF representation...\")\n",
    "X_tfidf = sklearn.feature_extraction.text.TfidfTransformer().fit_transform(data_vectorized.data)\n",
    "\n",
    "print(\"Creating NG3, NG6, and NG9 subsets...\")\n",
    "n_samples = 200\n",
    "\n",
    "ng3_cats = [\"comp.graphics\", \"rec.sport.baseball\", \"talk.politics.guns\"]\n",
    "ng6_cats = [\"alt.atheism\", \"comp.sys.mac.hardware\", \"rec.motorcycles\", \"rec.sport.hockey\", \"soc.religion.christian\", \"talk.religion.misc\"]\n",
    "ng9_cats = [\"talk.politics.mideast\", \"talk.politics.misc\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"sci.electronics\", \"sci.crypt\", \"sci.med\", \"sci.space\", \"misc.forsale\"]\n",
    "\n",
    "def create_ng_subset(ng_subset_cats):\n",
    "    ng_subset_data = []\n",
    "    for cat in ng_subset_cats:\n",
    "        cat_data = X_tfidf[data_vectorized[\"target\"] == data_vectorized[\"target_names\"].index(cat)]\n",
    "        idx = np.random.choice(cat_data.shape[0], n_samples, replace=False)\n",
    "        ng_subset_data.append(cat_data[idx])\n",
    "    ng_subset_data = scipy.sparse.vstack(ng_subset_data)\n",
    "    ng_subset_similarity = sklearn.metrics.pairwise.cosine_similarity(ng_subset_data, ng_subset_data)\n",
    "    NTS_ng_subset = ng_subset_similarity / np.sum(ng_subset_similarity, axis=1, keepdims=True)\n",
    "    Y_ng_subset = np.array([i for i in range(len(ng_subset_cats)) for _ in range(n_samples)])\n",
    "    return sklearn.utils.shuffle(NTS_ng_subset, Y_ng_subset)\n",
    "\n",
    "NTS_ng3, Y_ng3 = create_ng_subset(ng3_cats); print(NTS_ng3.shape, Y_ng3.shape)#; print(NTS_ng3[1][:5])\n",
    "NTS_ng6, Y_ng6 = create_ng_subset(ng6_cats); print(NTS_ng6.shape, Y_ng6.shape)#; print(NTS_ng6[1][:5])\n",
    "NTS_ng9, Y_ng9 = create_ng_subset(ng9_cats); print(NTS_ng9.shape, Y_ng9.shape)#; print(NTS_ng9[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51020952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NB_EPOCHS = 5000\n",
    "BATCH_SIZE = 178\n",
    "X = torch.tensor(NTS_ng3, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "graph_encoder = GraphEncoder(input_dim=X.shape[1], hidden_dims=[600, 512, 256]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(graph_encoder.parameters(), lr=0.01)\n",
    "nb_iters = NB_EPOCHS * (X.shape[0] // BATCH_SIZE)\n",
    "graph_encoder.train(X, compile=\"compile\", train_mode=\"layerwise\", iters=nb_iters, optimizer=optimizer, rho=0.01, beta=1.0, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Test\n",
    "latent = graph_encoder.encode(X).to('cpu')\n",
    "print(\"Latent shape:\", latent.shape)\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=len(set(Y_ng3)), random_state=42)\n",
    "y_pred = kmeans.fit_predict(latent)\n",
    "score = sklearn.metrics.normalized_mutual_info_score(Y_ng3, y_pred)\n",
    "print(f\"NMI: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e22011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NB_EPOCHS = 5000\n",
    "BATCH_SIZE = 178\n",
    "X = torch.tensor(NTS_ng6, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "graph_encoder = GraphEncoder(input_dim=X.shape[1], hidden_dims=[1200, 1024, 512, 256, 128]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(graph_encoder.parameters(), lr=0.01)\n",
    "nb_iters = NB_EPOCHS * (X.shape[0] // BATCH_SIZE)\n",
    "graph_encoder.train(X, compile=\"compile\", train_mode=\"layerwise\", iters=nb_iters, optimizer=optimizer, rho=0.01, beta=1.0, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Test\n",
    "latent = graph_encoder.encode(X).to('cpu')\n",
    "print(\"Latent shape:\", latent.shape)\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=len(set(Y_ng6)), random_state=42)\n",
    "y_pred = kmeans.fit_predict(latent)\n",
    "score = sklearn.metrics.normalized_mutual_info_score(Y_ng6, y_pred)\n",
    "print(f\"NMI: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d837de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NB_EPOCHS = 5000\n",
    "BATCH_SIZE = 178\n",
    "X = torch.tensor(NTS_ng9, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "graph_encoder = GraphEncoder(input_dim=X.shape[1], hidden_dims=[600, 512, 256]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(graph_encoder.parameters(), lr=0.01)\n",
    "nb_iters = NB_EPOCHS * (X.shape[0] // BATCH_SIZE)\n",
    "graph_encoder.train(X, compile=\"compile\", train_mode=\"layerwise\", iters=nb_iters, optimizer=optimizer, rho=0.01, beta=1.0, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Test\n",
    "latent = graph_encoder.encode(X).to('cpu')\n",
    "print(\"Latent shape:\", latent.shape)\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=len(set(Y_ng9)), random_state=42)\n",
    "y_pred = kmeans.fit_predict(latent)\n",
    "score = sklearn.metrics.normalized_mutual_info_score(Y_ng9, y_pred)\n",
    "print(f\"NMI: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e6488",
   "metadata": {},
   "source": [
    "## 2.3. DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9666e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aab8fedc",
   "metadata": {},
   "source": [
    "## BioGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b06a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
