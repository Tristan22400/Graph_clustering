{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d1d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younes/miniconda3/envs/ai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bea35",
   "metadata": {},
   "source": [
    "# 1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e580452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = torch.nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.sigmoid(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c200d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.autoencoders = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.autoencoders.append(AutoEncoder(prev_dim, hidden_dim))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        for autoencoder in self.autoencoders:\n",
    "            x = torch.sigmoid(autoencoder.encoder(x))\n",
    "        encoded = x\n",
    "        for autoencoder in reversed(self.autoencoders):\n",
    "            x = torch.sigmoid(autoencoder.decoder(x))\n",
    "        decoded = x\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0bc1a",
   "metadata": {},
   "source": [
    "# 2. Test on benchmark \"email\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c26a6",
   "metadata": {},
   "source": [
    "## 2.1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2468289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nts.shape: (1005, 1005)\n",
      "[*] number of clusters: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] original space average nmi: 0.35702058173203777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nxg = nx.read_gml(\"../datasets/reel/email/email.gml\") # read the email gml file into a networkx graph\n",
    "y = [nxg.nodes[n][\"value\"] for n in nxg.nodes] # extract the ground-truth community labels\n",
    "s = nx.to_numpy_array(nxg) # generate the similarity matrix\n",
    "s = s + np.diag(np.ones(nxg.number_of_nodes())) # we add self-loops (not indicated in the original paper but improves performance)\n",
    "nts = s / np.sum(s, axis=1, keepdims=True) # generate the normalized training set\n",
    "print(\"[*] nts.shape:\", nts.shape)\n",
    "print(\"[*] number of clusters:\", len(set(y)))\n",
    "cum = 0\n",
    "NB_KMEANS_TESTS = 20\n",
    "random.seed(0)\n",
    "for _ in tqdm.tqdm(range(NB_KMEANS_TESTS)):\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=len(set(y)), algorithm=\"lloyd\", random_state=random.randint(0, 10000))\n",
    "    y_pred_origspace = kmeans.fit_predict(nts)\n",
    "    cum += sklearn.metrics.normalized_mutual_info_score(y, y_pred_origspace)\n",
    "print(\"[*] original space average nmi:\", cum / NB_KMEANS_TESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e7062",
   "metadata": {},
   "source": [
    "## 2.2. Manual Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2953c13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 1: 100%|██████████| 5000/5000 [01:08<00:00, 73.24it/s]\n",
      "layer: 2: 100%|██████████| 5000/5000 [00:50<00:00, 99.04it/s] \n",
      "layer: 3: 100%|██████████| 5000/5000 [00:39<00:00, 125.26it/s]\n",
      "layer: 4: 100%|██████████| 5000/5000 [00:33<00:00, 150.31it/s]\n",
      "layer: 5: 100%|██████████| 5000/5000 [00:28<00:00, 175.34it/s]\n",
      "layer: 6: 100%|██████████| 5000/5000 [00:16<00:00, 299.09it/s]\n",
      "layer: 7: 100%|██████████| 5000/5000 [00:11<00:00, 435.23it/s]\n",
      "layer: 8: 100%|██████████| 5000/5000 [00:09<00:00, 528.86it/s]\n",
      "layer: 9: 100%|██████████| 5000/5000 [00:08<00:00, 601.82it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); print(\"device:\", DEVICE)\n",
    "BATCH_SIZE = 250\n",
    "HIDDEN_DIMS =  [950, 850, 750, 650, 550, 300, 170, 90, 45]\n",
    "\n",
    "# Create the model\n",
    "model = GraphEncoder(input_dim=nts.shape[1], hidden_dims=HIDDEN_DIMS).to(DEVICE)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create initial dataloader\n",
    "x_train = torch.tensor(nts, dtype=torch.float32).to(DEVICE)\n",
    "current_x_train = x_train.clone()\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(current_x_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "dataloader_iter = iter(dataloader)\n",
    "\n",
    "# Set some training parameters\n",
    "NB_EPOCHS_PER_LAYER = 1000\n",
    "nb_train_iters = NB_EPOCHS_PER_LAYER * len(dataloader)\n",
    "rho=0.01\n",
    "beta=1.0\n",
    "\n",
    "# Launch the training loop\n",
    "# For each layer in the stacked autoencoder: train the layer\n",
    "for layer_number in range(len(model.autoencoders)):\n",
    "    for _ in tqdm.tqdm(range(nb_train_iters), desc=f\"layer: {layer_number+1}\"):\n",
    "        try:\n",
    "            (x_batch,) = next(dataloader_iter)\n",
    "        except StopIteration:\n",
    "            dataloader_iter = iter(dataloader)\n",
    "            (x_batch,) = next(dataloader_iter)\n",
    "        optimizer.zero_grad()\n",
    "        encoded, decoded = model.autoencoders[layer_number](x_batch)\n",
    "        loss_1 = torch.nn.functional.mse_loss(decoded, x_batch, reduction='sum')\n",
    "        rho_hat = torch.mean(encoded, dim=0)\n",
    "        loss_2 = torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))\n",
    "        loss = loss_1 + beta * loss_2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Create new dataloader on the latent representations\n",
    "    with torch.no_grad():\n",
    "        latent_x_train, _ = model.autoencoders[layer_number](current_x_train)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(latent_x_train),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "        dataloader_iter = iter(dataloader)\n",
    "        current_x_train = latent_x_train.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db52ee7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 97.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] latent space nmi: 0.24720822940449344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on the latent space\n",
    "with torch.no_grad():\n",
    "    latent, _ = model(x_train)\n",
    "cum = 0\n",
    "NB_KMEANS_TESTS = 20\n",
    "for _ in tqdm.tqdm(range(NB_KMEANS_TESTS)):\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=len(set(y)), algorithm=\"lloyd\")\n",
    "    y_pred_latent = kmeans.fit_predict(latent.to('cpu'))\n",
    "    cum += sklearn.metrics.normalized_mutual_info_score(y, y_pred_latent)\n",
    "print(\"[*] latent space nmi:\", cum / NB_KMEANS_TESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5901e2a",
   "metadata": {},
   "source": [
    "## 2.3. Model training with hyper-parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a1ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trial 0----------------------------\n",
      "hidden dims = [804, 643]\n",
      "rho = 0.015702970884055395\n",
      "beta = 9.846738873614559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0:   0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 3000/3000 [00:21<00:00, 136.70it/s, loss=214]    \n",
      "layer: 1: 100%|██████████| 3000/3000 [00:12<00:00, 230.90it/s, loss=21.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] loss = 511.0034 (reconstruction: 490.7161, sparsity: 2.0603)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average nmi: 100%|██████████| 20/20 [00:02<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.20754237920575083\n",
      "\n",
      "trial 1----------------------------\n",
      "hidden dims = [603]\n",
      "rho = 0.00014936568554617635\n",
      "beta = 214.23021757741054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 3000/3000 [00:14<00:00, 202.36it/s, loss=6.49e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] loss = 7787.6238 (reconstruction: 831.2121, sparsity: 32.4717)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average nmi: 100%|██████████| 20/20 [00:02<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.19856419945182924\n",
      "\n",
      "trial 2----------------------------\n",
      "hidden dims = [804, 643, 514, 411]\n",
      "rho = 0.00011527987128232407\n",
      "beta = 707.2114131472224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 3000/3000 [00:18<00:00, 166.00it/s, loss=3.01e+4]\n",
      "layer: 1: 100%|██████████| 3000/3000 [00:12<00:00, 241.51it/s, loss=112]    \n",
      "layer: 2: 100%|██████████| 3000/3000 [00:08<00:00, 341.85it/s, loss=3.09e+4]\n",
      "layer: 3: 100%|██████████| 3000/3000 [00:06<00:00, 449.82it/s, loss=768]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] loss = 2490.9825 (reconstruction: 2243.7710, sparsity: 0.3496)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average nmi: 100%|██████████| 20/20 [00:00<00:00, 54.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.1608980721176483\n",
      "\n",
      "trial 3----------------------------\n",
      "hidden dims = [603, 361, 216, 129, 77]\n",
      "rho = 0.0003511356313970409\n",
      "beta = 0.08260808399079603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 3000/3000 [00:14<00:00, 206.65it/s, loss=19.8]\n",
      "layer: 1: 100%|██████████| 3000/3000 [00:06<00:00, 452.71it/s, loss=1.51]\n",
      "layer: 2: 100%|██████████| 3000/3000 [00:05<00:00, 585.65it/s, loss=5.42]\n",
      "layer: 3: 100%|██████████| 3000/3000 [00:04<00:00, 695.00it/s, loss=1.01] \n",
      "layer: 4: 100%|██████████| 3000/3000 [00:03<00:00, 772.36it/s, loss=2.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] loss = 171.7106 (reconstruction: 169.0903, sparsity: 31.7199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average nmi: 100%|██████████| 20/20 [00:00<00:00, 102.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.2159634628835377\n",
      "\n",
      "trial 4----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dims = [703, 492]\n",
      "rho = 0.0019762189340280074\n",
      "beta = 0.2858549394196191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 3000/3000 [00:15<00:00, 188.79it/s, loss=32.7]\n",
      "layer: 1: 100%|██████████| 3000/3000 [00:09<00:00, 326.62it/s, loss=4.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] loss = 195.7769 (reconstruction: 191.7770, sparsity: 13.9928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average nmi: 100%|██████████| 20/20 [00:02<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.1895940464059702\n",
      "\n",
      "trial 5----------------------------\n",
      "hidden dims = [603, 361, 216, 129]\n",
      "rho = 0.0007523742884534858\n",
      "beta = 0.6789053271698483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 3000/3000 [00:14<00:00, 206.73it/s, loss=51.1]\n",
      "layer: 1: 100%|██████████| 3000/3000 [00:06<00:00, 441.99it/s, loss=7.29]\n",
      "layer: 2: 100%|██████████| 3000/3000 [00:04<00:00, 604.77it/s, loss=19.4]\n",
      "layer: 3: 100%|██████████| 3000/3000 [00:04<00:00, 679.45it/s, loss=8.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] loss = 351.4947 (reconstruction: 343.2724, sparsity: 12.1110)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average nmi: 100%|██████████| 20/20 [00:00<00:00, 79.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.18843784631434987\n",
      "\n",
      "trial 6----------------------------\n",
      "hidden dims = [804, 643, 514]\n",
      "rho = 0.0003972110727381913\n",
      "beta = 3.725393839578884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 3000/3000 [00:18<00:00, 160.46it/s, loss=191]   \n",
      "layer: 1: 100%|██████████| 3000/3000 [00:14<00:00, 214.04it/s, loss=24.4]\n",
      "layer: 2: 100%|██████████| 3000/3000 [00:09<00:00, 313.57it/s, loss=59.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] loss = 2337.9276 (reconstruction: 2285.9788, sparsity: 13.9445)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average nmi: 100%|██████████| 20/20 [00:02<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.20820467713320276\n",
      "\n",
      "trial 7----------------------------\n",
      "hidden dims = [603, 361, 216]\n",
      "rho = 0.006647135865318031\n",
      "beta = 0.0712230583333387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 3000/3000 [00:14<00:00, 202.61it/s, loss=15.4]\n",
      "layer: 1: 100%|██████████| 3000/3000 [00:06<00:00, 451.60it/s, loss=1.04] \n",
      "layer: 2: 100%|██████████| 3000/3000 [00:04<00:00, 601.35it/s, loss=4.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] loss = 158.1226 (reconstruction: 154.1049, sparsity: 56.4092)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average nmi: 100%|██████████| 20/20 [00:00<00:00, 68.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.20762316391804805\n",
      "\n",
      "trial 8----------------------------\n",
      "hidden dims = [804]\n",
      "rho = 0.07886714129990492\n",
      "beta = 110.15056790269621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 3000/3000 [00:18<00:00, 163.32it/s, loss=208]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] loss = 344.8956 (reconstruction: 251.4968, sparsity: 0.8479)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average nmi: 100%|██████████| 20/20 [00:02<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.19867550755885655\n",
      "\n",
      "trial 9----------------------------\n",
      "hidden dims = [603, 361]\n",
      "rho = 0.01129013355909268\n",
      "beta = 1.587678152692399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 3000/3000 [00:14<00:00, 208.50it/s, loss=59.8]\n",
      "layer: 1: 100%|██████████| 3000/3000 [00:06<00:00, 472.61it/s, loss=9.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] loss = 325.4538 (reconstruction: 315.6394, sparsity: 6.1816)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average nmi: 100%|██████████| 20/20 [00:00<00:00, 47.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.21434911949799323\n",
      "========================================================\n",
      "========================================================\n",
      "[*] best avg nmi = 0.2159634628835377\n",
      "[*] best loss = 158.12258859370095\n",
      "[*] best loss avg nmi= 0.20762316391804805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Print trial number\n",
    "    print(f\"\\ntrial {trial.number}----------------------------\")\n",
    "    \n",
    "    # Set globals\n",
    "    global best_avg_nmi\n",
    "    global best_loss\n",
    "    global best_loss_avg_nmi\n",
    "    \n",
    "    # Set random seeds\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    # Suggest the number of layers and a decay rate for hidden dimensions\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5, step=1)\n",
    "    dim_decay_rate = trial.suggest_float(\"dim_decay_rate\", 0.6, 0.8, step=0.1)\n",
    "\n",
    "    # Compute the hidden dimensions\n",
    "    hidden_dims = []\n",
    "    prev_dim = x_train.shape[1]\n",
    "    for _ in range(n_layers):\n",
    "        next_dim = max(2, int(prev_dim * dim_decay_rate))\n",
    "        hidden_dims.append(next_dim)\n",
    "        prev_dim = next_dim\n",
    "    \n",
    "    # Create the model using the hidden dimensions\n",
    "    model = GraphEncoder(input_dim=x_train.shape[1], hidden_dims=hidden_dims).to(DEVICE)\n",
    "\n",
    "    # Suggest rho and beta for the sparsity constraint\n",
    "    rho = trial.suggest_float(\"rho\", 1e-4, 1e-1, log=True)\n",
    "    beta = trial.suggest_float(\"beta\", 1e-2, 1e3, log=True)\n",
    "    \n",
    "    # Suggest the optimizer (for now only AdamW is implemented)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"AdamW\"])\n",
    "    \n",
    "    # Create the optimizer based on the choice\n",
    "    match optimizer_name:\n",
    "        case \"AdamW\":\n",
    "            \n",
    "            # Suggest a learning rate\n",
    "            lr = trial.suggest_float(\"lr\", 1e-3, 1e-3, log=True)\n",
    "\n",
    "            # Suggest weight_decay for AdamW\n",
    "            weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-4, log=True)\n",
    "\n",
    "            # Create the optimizer\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Suggest batch size\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 34, 34)\n",
    "\n",
    "    # Create initial dataloader\n",
    "    current_x_train = x_train.clone().to(DEVICE)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(current_x_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    dataloader_iter = iter(dataloader)\n",
    "\n",
    "    nb_train_iters = NB_EPOCHS_PER_LAYER * len(dataloader)\n",
    "\n",
    "    # Print some hyper parameters\n",
    "    print(\"hidden dims =\", hidden_dims)\n",
    "    print(\"rho =\", rho)\n",
    "    print(\"beta =\", beta)\n",
    "    \n",
    "    # Launch the training loop\n",
    "    # For each layer in the stacked autoencoder: train the layer\n",
    "    for layer_number in range(len(model.autoencoders)):\n",
    "        for _ in (pb := tqdm.tqdm(range(nb_train_iters), desc=f\"layer: {layer_number}\")):\n",
    "            try:\n",
    "                (x_batch,) = next(dataloader_iter)\n",
    "            except StopIteration:\n",
    "                dataloader_iter = iter(dataloader)\n",
    "                (x_batch,) = next(dataloader_iter)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, decoded = model.autoencoders[layer_number](x_batch)\n",
    "            loss_1 = torch.nn.functional.mse_loss(decoded, x_batch, reduction='sum')\n",
    "            rho_hat = torch.mean(encoded, dim=0)\n",
    "            loss_2 = torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))\n",
    "            loss = loss_1 + beta * loss_2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pb.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        # Create new dataloader on the latent representations\n",
    "        with torch.no_grad():\n",
    "            current_x_train, _ = model.autoencoders[layer_number](current_x_train)\n",
    "            dataloader = torch.utils.data.DataLoader(\n",
    "                torch.utils.data.TensorDataset(current_x_train),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True\n",
    "            )\n",
    "            dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Evaluate loss\n",
    "        encoded, decoded = model(x_train)\n",
    "        loss_1 = torch.nn.functional.mse_loss(decoded, x_train, reduction='sum').item()\n",
    "        rho_hat = torch.mean(encoded, dim=0)\n",
    "        loss_2 = torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat))).item()\n",
    "        loss = loss_1 + beta * loss_2\n",
    "        print(f\"[*] loss = {loss:.4f} (reconstruction: {loss_1:.4f}, sparsity: {loss_2:.4f})\")\n",
    "        \n",
    "        # Evaluate average nmi\n",
    "        cum = 0\n",
    "        for _ in tqdm.tqdm(range(NB_KMEANS_TESTS), desc=\"average nmi\"):\n",
    "            kmeans = sklearn.cluster.KMeans(n_clusters=len(set(y)), algorithm=\"lloyd\", random_state=random.randint(0, 10000))\n",
    "            y_pred = kmeans.fit_predict(encoded)\n",
    "            cum += sklearn.metrics.normalized_mutual_info_score(y, y_pred)\n",
    "        avg_nmi = cum / NB_KMEANS_TESTS\n",
    "        print(\"[*] average nmi =\", avg_nmi)\n",
    "        if avg_nmi > best_avg_nmi:\n",
    "            best_avg_nmi = avg_nmi      \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_loss_avg_nmi = avg_nmi\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Create the optuna study\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(sampler=sampler,direction=\"minimize\")\n",
    "best_avg_nmi = 0.0\n",
    "best_loss = float('inf')\n",
    "best_loss_avg_nmi = 0.0\n",
    "x_train = torch.tensor(nts, dtype=torch.float32)\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NB_KMEANS_TESTS = 20\n",
    "NB_EPOCHS_PER_LAYER = 100\n",
    "\n",
    "# Run 10 trials\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Display the best results\n",
    "print(\"========================================================\")\n",
    "print(\"========================================================\")\n",
    "print(\"[*] best avg nmi =\", best_avg_nmi)\n",
    "print(\"[*] best loss =\", best_loss)\n",
    "print(\"[*] best loss avg nmi=\", best_loss_avg_nmi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
