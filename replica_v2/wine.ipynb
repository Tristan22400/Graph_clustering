{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d1d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younes/miniconda3/envs/ai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "import random\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bea35",
   "metadata": {},
   "source": [
    "# 1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e580452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = torch.nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.sigmoid(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c200d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.autoencoders = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.autoencoders.append(AutoEncoder(prev_dim, hidden_dim))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        for autoencoder in self.autoencoders:\n",
    "            x = torch.sigmoid(autoencoder.encoder(x))\n",
    "        encoded = x\n",
    "        for autoencoder in reversed(self.autoencoders):\n",
    "            x = torch.sigmoid(autoencoder.decoder(x))\n",
    "        decoded = x\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0bc1a",
   "metadata": {},
   "source": [
    "# 2. Test on benchmark \"Wine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ncut(s, labels):\n",
    "    \"\"\"\n",
    "    Compute  normalized cut for given similarity matrix s and cluster labels:\n",
    "      Ncut = sum_k cut(C_k, V\\C_k) / assoc(C_k, V)\n",
    "    where\n",
    "      cut(C, V\\C) = sum_{i in C, j not in C} A[i,j]\n",
    "      assoc(C, V) = sum_{i in C, j in V} A[i,j]  (i.e., volume of C)\n",
    "    A : symmetric adjacency/similarity numpy array\n",
    "    labels : length-n array of integer cluster labels\n",
    "    Returns float Ncut value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the unique labels in the community assignment\n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    # Precompute degrees\n",
    "    degrees = s.sum(axis=1)  # degree/volume per node\n",
    "    \n",
    "    # Initialize ncut\n",
    "    ncut = 0.0\n",
    "    \n",
    "    # For each cluster compute link and volume, then sum up to get ncut\n",
    "    for lab in unique_labels:\n",
    "        \n",
    "        # Get the indices of nodes in cluster lab\n",
    "        idx = np.where(labels == lab)[0]\n",
    "        if idx.size == 0:\n",
    "            raise Exception(\"compute_ncut_from_labels: empty cluster found in labels.\")\n",
    "        \n",
    "        # Compute volume = sum of degrees of nodes in idx\n",
    "        volume = degrees[idx].sum()\n",
    "        \n",
    "        # If volume is not zero, compute link to get the local cut then sum to ncut, otherwise skip (i.e. cut = 0)\n",
    "        if volume != 0:\n",
    "\n",
    "            # Compute link = sum over i in C, j not in C, of A[i,j]\n",
    "            # = volume - internal connections\n",
    "            internal_connections = s[np.ix_(idx, idx)].sum()\n",
    "            link = volume - internal_connections\n",
    "            \n",
    "            # Compute local cut contribution\n",
    "            local_cut = link / volume\n",
    "\n",
    "            # Sum to ncut\n",
    "            ncut += local_cut\n",
    "    \n",
    "    return ncut\n",
    "\n",
    "warnings.filterwarnings(\"error\", category=sklearn.exceptions.ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c26a6",
   "metadata": {},
   "source": [
    "## 2.1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd42d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nts.shape: (178, 178)\n",
      "[*] number of clusters: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 284.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] original space average nmi: 0.6351524906645806\n",
      "[*] original space average ncut: 1.898660189065159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading Wine\n",
    "x, y= sklearn.datasets.load_wine(return_X_y=True, as_frame=False)\n",
    "x = sklearn.preprocessing.MinMaxScaler().fit_transform(x)\n",
    "s = sklearn.metrics.pairwise.cosine_similarity(x, x)\n",
    "nts = s / np.sum(s, axis=1, keepdims=True)\n",
    "print(\"[*] nts.shape:\", nts.shape)\n",
    "print(\"[*] number of clusters:\", len(set(y)))\n",
    "cumulated_nmi = 0\n",
    "cumulated_ncut = 0\n",
    "nb_kmeans_tests = 100\n",
    "random.seed(0)\n",
    "for _ in tqdm.tqdm(range(nb_kmeans_tests)):\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=len(set(y)), algorithm=\"lloyd\", random_state=random.randint(0, 10000))\n",
    "    y_pred_origspace = kmeans.fit_predict(nts)\n",
    "    cumulated_nmi += sklearn.metrics.normalized_mutual_info_score(y, y_pred_origspace)\n",
    "    cumulated_ncut += compute_ncut(s, y_pred_origspace)\n",
    "print(\"[*] original space average nmi:\", cumulated_nmi / nb_kmeans_tests)\n",
    "print(\"[*] original space average ncut:\", cumulated_ncut / nb_kmeans_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2b8faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 98.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] original space average nmi: 0.7129036297699397\n",
      "[*] original space average ncut: 1.8964924406149422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cumulated_nmi = 0\n",
    "cumulated_ncut = 0\n",
    "nb_kmeans_tests = 100\n",
    "random.seed(0)\n",
    "for _ in tqdm.tqdm(range(nb_kmeans_tests)):\n",
    "    y_pred_origspace = y_pred_origspace = sklearn.cluster.SpectralClustering(n_clusters=len(set(y)), affinity='precomputed', assign_labels='kmeans', random_state=random.randint(0, 10000)).fit_predict(s)\n",
    "    cumulated_nmi += sklearn.metrics.normalized_mutual_info_score(y, y_pred_origspace)\n",
    "    cumulated_ncut += compute_ncut(s, y_pred_origspace)\n",
    "print(\"[*] original space average nmi:\", cumulated_nmi / nb_kmeans_tests)\n",
    "print(\"[*] original space average ncut:\", cumulated_ncut / nb_kmeans_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a1ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] using device: cuda\n",
      "\n",
      "trial 0----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.0013292918943162175\n",
      "> beta = 566.9849511478844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 1000/1000 [00:03<00:00, 330.26it/s, loss=1.19e+3]\n",
      "layer: 1: 100%|██████████| 1000/1000 [00:02<00:00, 364.74it/s, loss=224]   \n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 377.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.260493922075894\n",
      "[*] average ncut = 1.9609009154117445\n",
      "\n",
      "trial 1----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.00029380279387035364\n",
      "> beta = 0.060252157362038566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 5000/5000 [00:14<00:00, 346.79it/s, loss=4.71]\n",
      "layer: 1: 100%|██████████| 5000/5000 [00:14<00:00, 345.17it/s, loss=0.00548]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 415.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.5118703712076803\n",
      "[*] average ncut = 1.9158383986290632\n",
      "\n",
      "trial 2----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.006358358856676255\n",
      "> beta = 34.70266988650411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 5000/5000 [00:13<00:00, 373.41it/s, loss=80.1]  \n",
      "layer: 1: 100%|██████████| 5000/5000 [00:12<00:00, 415.66it/s, loss=38.9]  \n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 394.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.3621343465190873\n",
      "[*] average ncut = 1.9482073586200852\n",
      "\n",
      "trial 3----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.03142880890840111\n",
      "> beta = 0.1152644954031561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 100/100 [00:00<00:00, 346.70it/s, loss=43.7]\n",
      "layer: 1: 100%|██████████| 100/100 [00:00<00:00, 376.59it/s, loss=0.842]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 424.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6826025652417439\n",
      "[*] average ncut = 1.8967682067285856\n",
      "\n",
      "trial 4----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.0008179499475211679\n",
      "> beta = 4.205156450913866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 100/100 [00:00<00:00, 373.86it/s, loss=368]\n",
      "layer: 1: 100%|██████████| 100/100 [00:00<00:00, 354.32it/s, loss=3.06]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 402.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6885307113948802\n",
      "[*] average ncut = 1.9005745068756064\n",
      "\n",
      "trial 5----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.006847920095574782\n",
      "> beta = 0.04982752357076448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 500/500 [00:01<00:00, 375.54it/s, loss=6.17]\n",
      "layer: 1: 100%|██████████| 500/500 [00:01<00:00, 376.95it/s, loss=0.363]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 452.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6915184550496444\n",
      "[*] average ncut = 1.8970448997805838\n",
      "\n",
      "trial 6----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.0023345864076016252\n",
      "> beta = 84.31013932082456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 1000/1000 [00:02<00:00, 381.48it/s, loss=1.26e+3]\n",
      "layer: 1: 100%|██████████| 1000/1000 [00:02<00:00, 380.57it/s, loss=224]   \n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 415.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6941421534308937\n",
      "[*] average ncut = 1.9062824930715307\n",
      "\n",
      "trial 7----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.005987474910461402\n",
      "> beta = 0.017070728830306643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 100/100 [00:00<00:00, 382.29it/s, loss=2.5]\n",
      "layer: 1: 100%|██████████| 100/100 [00:00<00:00, 375.17it/s, loss=0.092]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 449.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.667359010181858\n",
      "[*] average ncut = 1.8975897524119185\n",
      "\n",
      "trial 8----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.00015673095467235422\n",
      "> beta = 555.1721685244722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 2500/2500 [00:06<00:00, 379.52it/s, loss=80.7]  \n",
      "layer: 1: 100%|██████████| 2500/2500 [00:07<00:00, 347.03it/s, loss=103]   \n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 297.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.2662524760431544\n",
      "[*] average ncut = 1.960358100939988\n",
      "\n",
      "trial 9----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.0008200518402245837\n",
      "> beta = 0.030786517836196188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 500/500 [00:01<00:00, 352.75it/s, loss=3.05]\n",
      "layer: 1: 100%|██████████| 500/500 [00:01<00:00, 327.58it/s, loss=0.0976]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 417.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6874082001802809\n",
      "[*] average ncut = 1.8986060967680252\n",
      "\n",
      "trial 10----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.06845570267272912\n",
      "> beta = 0.6586252373799878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 10/10 [00:00<00:00, 332.84it/s, loss=1.5e+3]\n",
      "layer: 1: 100%|██████████| 10/10 [00:00<00:00, 365.14it/s, loss=10.6]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 412.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6634471326481035\n",
      "[*] average ncut = 1.897359233767765\n",
      "\n",
      "trial 11----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.029137736730843713\n",
      "> beta = 0.3182908215905923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 100/100 [00:00<00:00, 304.83it/s, loss=48.8]\n",
      "layer: 1: 100%|██████████| 100/100 [00:00<00:00, 276.56it/s, loss=1.27]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 334.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6820624256266656\n",
      "[*] average ncut = 1.8968453373301457\n",
      "\n",
      "trial 12----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.04993961824486366\n",
      "> beta = 0.55431031121428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 10/10 [00:00<00:00, 235.48it/s, loss=2.75e+3]\n",
      "layer: 1: 100%|██████████| 10/10 [00:00<00:00, 333.51it/s, loss=14.4]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 441.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6636746754798887\n",
      "[*] average ncut = 1.8973527187934016\n",
      "\n",
      "trial 13----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.024820762449519945\n",
      "> beta = 0.40276065491042357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 100/100 [00:00<00:00, 363.77it/s, loss=68.2]\n",
      "layer: 1: 100%|██████████| 100/100 [00:00<00:00, 360.44it/s, loss=1.85]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 349.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6783239712402153\n",
      "[*] average ncut = 1.896417754703889\n",
      "\n",
      "trial 14----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.01985405367141798\n",
      "> beta = 3.4547878435335257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 100/100 [00:00<00:00, 287.92it/s, loss=340]   \n",
      "layer: 1: 100%|██████████| 100/100 [00:00<00:00, 318.39it/s, loss=3.65]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 347.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.7081083835306905\n",
      "[*] average ncut = 1.8970130205580091\n",
      "\n",
      "trial 15----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.014994997980847432\n",
      "> beta = 0.15997042090749544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 10/10 [00:00<00:00, 253.19it/s, loss=664]\n",
      "layer: 1: 100%|██████████| 10/10 [00:00<00:00, 271.75it/s, loss=9.17]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 364.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6593840271374968\n",
      "[*] average ncut = 1.8972427729353842\n",
      "\n",
      "trial 16----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.08840029696246197\n",
      "> beta = 1.6009404998554393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 500/500 [00:01<00:00, 316.87it/s, loss=66.4]\n",
      "layer: 1: 100%|██████████| 500/500 [00:01<00:00, 317.08it/s, loss=0.000637]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 375.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6983101583077843\n",
      "[*] average ncut = 1.8970389086127213\n",
      "\n",
      "trial 17----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.015413721074870875\n",
      "> beta = 0.010415610209035201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 100/100 [00:00<00:00, 315.09it/s, loss=47.2]\n",
      "layer: 1: 100%|██████████| 100/100 [00:00<00:00, 320.07it/s, loss=0.155]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 375.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6769578707111265\n",
      "[*] average ncut = 1.8968657880161661\n",
      "\n",
      "trial 18----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.03934866506014746\n",
      "> beta = 20.750670919082573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 500/500 [00:01<00:00, 361.21it/s, loss=241]   \n",
      "layer: 1: 100%|██████████| 500/500 [00:01<00:00, 316.39it/s, loss=38.5]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 420.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.6516708494718527\n",
      "[*] average ncut = 1.9084256127998076\n",
      "\n",
      "trial 19----------------------------\n",
      "> hidden dims = [128, 64]\n",
      "> rho = 0.010795471446152866\n",
      "> beta = 0.11855939250721566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 10/10 [00:00<00:00, 271.49it/s, loss=33.7]\n",
      "layer: 1: 100%|██████████| 10/10 [00:00<00:00, 257.45it/s, loss=5.51]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 341.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.652153713229684\n",
      "[*] average ncut = 1.8980507870545607\n",
      "========================================================\n",
      "========================================================\n",
      "[*] best avg nmi = 0.7081083835306905\n",
      "[*] best avg ncut = 1.896417754703889\n",
      "[*] best avg ncut avg nmi = 0.6783239712402153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Print trial number\n",
    "    print(f\"\\ntrial {trial.number}----------------------------\")\n",
    "    \n",
    "    # Set globals\n",
    "    global best_avg_nmi\n",
    "    global best_avg_ncut\n",
    "    global best_avg_ncut_avg_nmi\n",
    "    \n",
    "    # Set random seeds\n",
    "    torch.manual_seed(97)\n",
    "    np.random.seed(97)\n",
    "    random.seed(97)\n",
    "    \n",
    "    # Create the model using the hidden dimensions\n",
    "    hidden_dims = [128, 64]\n",
    "    model = GraphEncoder(input_dim=x_train.shape[1], hidden_dims=[128,64]).to(device)\n",
    "\n",
    "    # Suggest rho and beta for the sparsity constraint\n",
    "    rho = trial.suggest_float(\"rho\", 1e-4, 1e-1, log=True)\n",
    "    beta = trial.suggest_float(\"beta\", 1e-2, 1e3, log=True)\n",
    "    \n",
    "    # Suggest a learning rate for the optimizer and create the optimizer    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-3, 1e-2, log=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Create initial dataloader\n",
    "    current_x_train = x_train.clone().to(device)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(current_x_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    # Suggest nb_epochs_per_layer\n",
    "    nb_epochs_per_layer = nb_epochs_per_layer_pool[trial.suggest_int(\"nb_epochs_per_layer\", 0, len(nb_epochs_per_layer_pool)-1)]\n",
    "    nb_train_iters = nb_epochs_per_layer * len(dataloader)\n",
    "\n",
    "    # Print some hyper parameters\n",
    "    print(\"> hidden dims =\", hidden_dims)\n",
    "    print(\"> rho =\", rho)\n",
    "    print(\"> beta =\", beta)\n",
    "    \n",
    "    # Launch the training loop\n",
    "    # For each layer in the stacked autoencoder: train the layer\n",
    "    for layer_number in range(len(model.autoencoders)):\n",
    "        for _ in (pb := tqdm.tqdm(range(nb_train_iters), desc=f\"layer: {layer_number}\")):\n",
    "            try:\n",
    "                (x_batch,) = next(dataloader_iter)\n",
    "            except StopIteration:\n",
    "                dataloader_iter = iter(dataloader)\n",
    "                (x_batch,) = next(dataloader_iter)\n",
    "            x_batch = x_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, decoded = model.autoencoders[layer_number](x_batch)\n",
    "            loss_1 = torch.nn.functional.mse_loss(decoded, x_batch, reduction='sum')\n",
    "            rho_hat = torch.mean(encoded, dim=0)\n",
    "            loss_2 = torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))\n",
    "            loss = loss_1 + beta * loss_2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pb.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        # Create new dataloader on the latent representations\n",
    "        with torch.no_grad():\n",
    "            current_x_train, _ = model.autoencoders[layer_number](current_x_train)\n",
    "            dataloader = torch.utils.data.DataLoader(\n",
    "                torch.utils.data.TensorDataset(current_x_train),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True\n",
    "            )\n",
    "            dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    try:\n",
    "        # Evaluate the model\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Get the encoded representations\n",
    "            encoded, _ = model(x_train)\n",
    "            encoded = encoded.to('cpu')\n",
    "            \n",
    "            # Evaluate average nmi and ncut over several kmeans runs\n",
    "            cumulated_nmi = 0\n",
    "            cumulated_ncut = 0\n",
    "            for _ in tqdm.tqdm(range(nb_kmeans_tests), desc=\"computing avg nmi and ncut\"):\n",
    "                kmeans = sklearn.cluster.KMeans(n_clusters=len(set(y)), algorithm=\"lloyd\", random_state=random.randint(0, 10000,), n_init='auto')\n",
    "                y_pred = kmeans.fit_predict(encoded)\n",
    "                cumulated_nmi += sklearn.metrics.normalized_mutual_info_score(y, y_pred)\n",
    "                cumulated_ncut += compute_ncut(s, y_pred)\n",
    "            avg_nmi = cumulated_nmi / nb_kmeans_tests\n",
    "            avg_ncut = cumulated_ncut / nb_kmeans_tests\n",
    "            \n",
    "            # Print average nmi and ncut\n",
    "            print(\"[*] average nmi =\", avg_nmi)\n",
    "            print(\"[*] average ncut =\", avg_ncut)\n",
    "            \n",
    "            # If average nmi is better than the best so far, update best_avg_nmi\n",
    "            if avg_nmi > best_avg_nmi:\n",
    "                best_avg_nmi = avg_nmi\n",
    "            \n",
    "            # If average ncut is better than the best so far, update best_avg_ncut and its corresponding average nmi (i.e. best_avg_ncut_avg_nmi)\n",
    "            if avg_ncut < best_avg_ncut:\n",
    "                best_avg_ncut = avg_ncut\n",
    "                best_avg_ncut_avg_nmi = avg_nmi\n",
    "    \n",
    "    except sklearn.exceptions.ConvergenceWarning:\n",
    "        print(\"[!] KMeans did not converge (not enough distinct points) --> Returning inf for avg_ncut\")\n",
    "        avg_ncut = float('inf')\n",
    "\n",
    "    # Return avg_ncut as the objective to minimize\n",
    "    return avg_ncut\n",
    "\n",
    "\n",
    "# Set global parameters\n",
    "nb_epochs_per_layer_pool = [10, 100, 500, 1000, 2500, 5000]\n",
    "nb_kmeans_tests = 100\n",
    "nb_trials = 20\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu'); print(\"[*] using device:\", device)\n",
    "x_train = torch.tensor(nts, dtype=torch.float32).to(device)\n",
    "batch_size = x_train.shape[0]\n",
    "\n",
    "# Set globals to track best results\n",
    "best_avg_nmi = 0.0\n",
    "best_avg_ncut = float('inf')\n",
    "best_avg_ncut_avg_nmi = 0.0\n",
    "\n",
    "# Run optuna study\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(sampler=sampler, direction=\"minimize\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study.optimize(objective, n_trials=nb_trials)\n",
    "\n",
    "# Display the best results\n",
    "print(\"========================================================\")\n",
    "print(\"========================================================\")\n",
    "print(\"[*] best avg nmi =\", best_avg_nmi)\n",
    "print(\"[*] best avg ncut =\", best_avg_ncut)\n",
    "print(\"[*] best avg ncut avg nmi =\", best_avg_ncut_avg_nmi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
