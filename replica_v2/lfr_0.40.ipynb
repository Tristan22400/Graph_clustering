{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d1d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younes/miniconda3/envs/ai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "import random\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bea35",
   "metadata": {},
   "source": [
    "# 1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e580452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = torch.nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.sigmoid(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c200d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.autoencoders = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.autoencoders.append(AutoEncoder(prev_dim, hidden_dim))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        for autoencoder in self.autoencoders:\n",
    "            x = torch.sigmoid(autoencoder.encoder(x))\n",
    "        encoded = x\n",
    "        for autoencoder in reversed(self.autoencoders):\n",
    "            x = torch.sigmoid(autoencoder.decoder(x))\n",
    "        decoded = x\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0bc1a",
   "metadata": {},
   "source": [
    "# 2. Test on benchmark \"football\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c79194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ncut(s, labels):\n",
    "    \"\"\"\n",
    "    Compute  normalized cut for given similarity matrix s and cluster labels:\n",
    "      Ncut = sum_k cut(C_k, V\\C_k) / assoc(C_k, V)\n",
    "    where\n",
    "      cut(C, V\\C) = sum_{i in C, j not in C} A[i,j]\n",
    "      assoc(C, V) = sum_{i in C, j in V} A[i,j]  (i.e., volume of C)\n",
    "    A : symmetric adjacency/similarity numpy array\n",
    "    labels : length-n array of integer cluster labels\n",
    "    Returns float Ncut value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the unique labels in the community assignment\n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    # Precompute degrees\n",
    "    degrees = s.sum(axis=1)  # degree/volume per node\n",
    "    \n",
    "    # Initialize ncut\n",
    "    ncut = 0.0\n",
    "    \n",
    "    # For each cluster compute link and volume, then sum up to get ncut\n",
    "    for lab in unique_labels:\n",
    "        \n",
    "        # Get the indices of nodes in cluster lab\n",
    "        idx = np.where(labels == lab)[0]\n",
    "        if idx.size == 0:\n",
    "            raise Exception(\"compute_ncut_from_labels: empty cluster found in labels.\")\n",
    "        \n",
    "        # Compute volume = sum of degrees of nodes in idx\n",
    "        volume = degrees[idx].sum()\n",
    "        \n",
    "        # If volume is not zero, compute link to get the local cut then sum to ncut, otherwise skip (i.e. cut = 0)\n",
    "        if volume != 0:\n",
    "\n",
    "            # Compute link = sum over i in C, j not in C, of A[i,j]\n",
    "            # = volume - internal connections\n",
    "            internal_connections = s[np.ix_(idx, idx)].sum()\n",
    "            link = volume - internal_connections\n",
    "            \n",
    "            # Compute local cut contribution\n",
    "            local_cut = link / volume\n",
    "\n",
    "            # Sum to ncut\n",
    "            ncut += local_cut\n",
    "    \n",
    "    return ncut\n",
    "\n",
    "warnings.filterwarnings(\"error\", category=sklearn.exceptions.ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c26a6",
   "metadata": {},
   "source": [
    "## 2.1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2468289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nts.shape: (250, 250)\n",
      "[*] number of clusters: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 115.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] original space average nmi: 0.01319438903360806\n",
      "[*] original space average ncut: 1.221212088695503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nxg = nx.read_gml(\"../datasets/synthetic/lfr_0.40.gml\") # read the football gml file into a networkx graph\n",
    "y = [nxg.nodes[n][\"value\"] for n in nxg.nodes] # extract the ground-truth community labels\n",
    "s = nx.to_numpy_array(nxg) # generate the similarity matrix\n",
    "s = s + np.diag(np.ones(nxg.number_of_nodes())) # we add self-loops (not indicated in the original paper but improves performance)\n",
    "nts = s / np.sum(s, axis=1, keepdims=True) # generate the normalized training set\n",
    "print(\"[*] nts.shape:\", nts.shape)\n",
    "print(\"[*] number of clusters:\", len(set(y)))\n",
    "cumulated_nmi = 0\n",
    "cumulated_ncut = 0\n",
    "nb_kmeans_tests = 100\n",
    "random.seed(0)\n",
    "for _ in tqdm.tqdm(range(nb_kmeans_tests)):\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=len(set(y)), algorithm=\"lloyd\", random_state=random.randint(0, 10000))\n",
    "    y_pred_origspace = kmeans.fit_predict(nts)\n",
    "    cumulated_nmi += sklearn.metrics.normalized_mutual_info_score(y, y_pred_origspace)\n",
    "    cumulated_ncut += compute_ncut(s, y_pred_origspace)\n",
    "print(\"[*] original space average nmi:\", cumulated_nmi / nb_kmeans_tests)\n",
    "print(\"[*] original space average ncut:\", cumulated_ncut / nb_kmeans_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62bd845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 27.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] original space average nmi: 0.0097685375855284\n",
      "[*] original space average ncut: 0.5515753822934782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Graph is not fully connected, spectral embedding may not work as expected.\")\n",
    "cumulated_nmi = 0\n",
    "cumulated_ncut = 0\n",
    "nb_kmeans_tests = 100\n",
    "random.seed(0)\n",
    "for _ in tqdm.tqdm(range(nb_kmeans_tests)):\n",
    "    y_pred_origspace = y_pred_origspace = sklearn.cluster.SpectralClustering(n_clusters=len(set(y)), affinity='precomputed', assign_labels='kmeans', random_state=random.randint(0, 10000)).fit_predict(s)\n",
    "    cumulated_nmi += sklearn.metrics.normalized_mutual_info_score(y, y_pred_origspace)\n",
    "    cumulated_ncut += compute_ncut(s, y_pred_origspace)\n",
    "print(\"[*] original space average nmi:\", cumulated_nmi / nb_kmeans_tests)\n",
    "print(\"[*] original space average ncut:\", cumulated_ncut / nb_kmeans_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5901e2a",
   "metadata": {},
   "source": [
    "## 2.3. Model training with hyper-parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a1ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 09:25:17,122] A new study created in memory with name: no-name-32d11d69-2514-4363-9071-f82cb3d25511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trial 0----------------------------\n",
      "> hidden dims = [175, 122, 85, 59, 41, 28, 19, 13, 9, 6, 4]\n",
      "> rho = 0.015702970884055395\n",
      "> beta = 9.846738873614559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 10/10 [00:00<00:00, 17.12it/s, loss=5.29e+3]\n",
      "layer: 1: 100%|██████████| 10/10 [00:00<00:00, 113.23it/s, loss=300]\n",
      "layer: 2: 100%|██████████| 10/10 [00:00<00:00, 113.55it/s, loss=1.2e+3]\n",
      "layer: 3: 100%|██████████| 10/10 [00:00<00:00, 118.07it/s, loss=310]\n",
      "layer: 4: 100%|██████████| 10/10 [00:00<00:00, 118.64it/s, loss=328]\n",
      "layer: 5: 100%|██████████| 10/10 [00:00<00:00, 123.83it/s, loss=243]\n",
      "layer: 6: 100%|██████████| 10/10 [00:00<00:00, 119.78it/s, loss=166]\n",
      "layer: 7: 100%|██████████| 10/10 [00:00<00:00, 100.55it/s, loss=108]\n",
      "layer: 8: 100%|██████████| 10/10 [00:00<00:00, 98.58it/s, loss=75.2]\n",
      "layer: 9: 100%|██████████| 10/10 [00:00<00:00, 112.38it/s, loss=57]\n",
      "layer: 10: 100%|██████████| 10/10 [00:00<00:00, 117.63it/s, loss=43.8]\n",
      "computing avg nmi and ncut:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] KMeans did not converge (not enough distinct points) --> Returning inf for avg_ncut\n",
      "\n",
      "trial 1----------------------------\n",
      "> hidden dims = [150, 90, 54, 32, 19, 11, 6]\n",
      "> rho = 0.006358358856676255\n",
      "> beta = 34.70266988650411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 5000/5000 [00:44<00:00, 113.55it/s, loss=198]    \n",
      "layer: 1: 100%|██████████| 5000/5000 [00:16<00:00, 305.53it/s, loss=42.2]  \n",
      "layer: 2: 100%|██████████| 5000/5000 [00:14<00:00, 334.25it/s, loss=57]    \n",
      "layer: 3: 100%|██████████| 5000/5000 [00:15<00:00, 332.55it/s, loss=38]    \n",
      "layer: 4: 100%|██████████| 5000/5000 [00:15<00:00, 330.81it/s, loss=23.8] \n",
      "layer: 5: 100%|██████████| 5000/5000 [00:15<00:00, 312.72it/s, loss=17]   \n",
      "layer: 6: 100%|██████████| 5000/5000 [00:15<00:00, 317.90it/s, loss=10.5]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 400.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.006363918161098709\n",
      "[*] average ncut = 1.4877014035979175\n",
      "\n",
      "trial 2----------------------------\n",
      "> hidden dims = [212, 180, 153, 130, 110]\n",
      "> rho = 0.0003511356313970409\n",
      "> beta = 0.08260808399079603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 1000/1000 [00:02<00:00, 334.41it/s, loss=64.4]\n",
      "layer: 1: 100%|██████████| 1000/1000 [00:03<00:00, 311.95it/s, loss=1.93]\n",
      "layer: 2: 100%|██████████| 1000/1000 [00:02<00:00, 335.67it/s, loss=10.8]\n",
      "layer: 3: 100%|██████████| 1000/1000 [00:02<00:00, 348.31it/s, loss=0.486]\n",
      "layer: 4: 100%|██████████| 1000/1000 [00:02<00:00, 349.87it/s, loss=8.7]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 364.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.004439682635058223\n",
      "[*] average ncut = 1.3218674932922823\n",
      "\n",
      "trial 3----------------------------\n",
      "> hidden dims = [187, 140, 105, 78, 58]\n",
      "> rho = 0.006847920095574782\n",
      "> beta = 0.04982752357076448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 500/500 [00:01<00:00, 344.61it/s, loss=61.1]\n",
      "layer: 1: 100%|██████████| 500/500 [00:01<00:00, 320.36it/s, loss=1.7] \n",
      "layer: 2: 100%|██████████| 500/500 [00:01<00:00, 337.30it/s, loss=4.67]\n",
      "layer: 3: 100%|██████████| 500/500 [00:01<00:00, 305.22it/s, loss=0.862]\n",
      "layer: 4: 100%|██████████| 500/500 [00:01<00:00, 318.08it/s, loss=2.78]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 308.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.009438542752586815\n",
      "[*] average ncut = 1.3702278439062812\n",
      "\n",
      "trial 4----------------------------\n",
      "> hidden dims = [187, 140, 105, 78, 58, 43, 32, 24, 18, 13, 9]\n",
      "> rho = 0.0003972110727381913\n",
      "> beta = 3.725393839578884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 10/10 [00:00<00:00, 311.45it/s, loss=755]\n",
      "layer: 1: 100%|██████████| 10/10 [00:00<00:00, 365.17it/s, loss=40.1]\n",
      "layer: 2: 100%|██████████| 10/10 [00:00<00:00, 359.88it/s, loss=657]\n",
      "layer: 3: 100%|██████████| 10/10 [00:00<00:00, 370.33it/s, loss=56]\n",
      "layer: 4: 100%|██████████| 10/10 [00:00<00:00, 375.97it/s, loss=502]\n",
      "layer: 5: 100%|██████████| 10/10 [00:00<00:00, 323.08it/s, loss=58]\n",
      "layer: 6: 100%|██████████| 10/10 [00:00<00:00, 383.29it/s, loss=272]\n",
      "layer: 7: 100%|██████████| 10/10 [00:00<00:00, 341.47it/s, loss=62.3]\n",
      "layer: 8: 100%|██████████| 10/10 [00:00<00:00, 338.33it/s, loss=117]\n",
      "layer: 9: 100%|██████████| 10/10 [00:00<00:00, 355.54it/s, loss=90.3]\n",
      "layer: 10: 100%|██████████| 10/10 [00:00<00:00, 339.77it/s, loss=45]\n",
      "computing avg nmi and ncut:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] KMeans did not converge (not enough distinct points) --> Returning inf for avg_ncut\n",
      "\n",
      "trial 5----------------------------\n",
      "> hidden dims = [200, 160, 128, 102]\n",
      "> rho = 0.00015673095467235422\n",
      "> beta = 555.1721685244722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 2500/2500 [00:07<00:00, 334.55it/s, loss=182]   \n",
      "layer: 1: 100%|██████████| 2500/2500 [00:07<00:00, 337.14it/s, loss=221]    \n",
      "layer: 2: 100%|██████████| 2500/2500 [00:07<00:00, 334.88it/s, loss=143]   \n",
      "layer: 3: 100%|██████████| 2500/2500 [00:07<00:00, 343.49it/s, loss=140]   \n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 342.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.012651105129518376\n",
      "[*] average ncut = 1.5279956223581628\n",
      "\n",
      "trial 6----------------------------\n",
      "> hidden dims = [175, 122]\n",
      "> rho = 0.01129013355909268\n",
      "> beta = 1.587678152692399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 500/500 [00:01<00:00, 302.83it/s, loss=234]\n",
      "layer: 1: 100%|██████████| 500/500 [00:01<00:00, 332.18it/s, loss=1]   \n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 349.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.008349802906416992\n",
      "[*] average ncut = 1.3404911067665544\n",
      "\n",
      "trial 7----------------------------\n",
      "> hidden dims = [150, 90, 54, 32, 19, 11, 6, 3]\n",
      "> rho = 0.0005975027999960298\n",
      "> beta = 20.54051942538844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 1000/1000 [00:02<00:00, 339.45it/s, loss=561]   \n",
      "layer: 1: 100%|██████████| 1000/1000 [00:02<00:00, 338.73it/s, loss=95.4]\n",
      "layer: 2: 100%|██████████| 1000/1000 [00:03<00:00, 322.48it/s, loss=220]  \n",
      "layer: 3: 100%|██████████| 1000/1000 [00:03<00:00, 305.14it/s, loss=79.7]\n",
      "layer: 4: 100%|██████████| 1000/1000 [00:03<00:00, 324.41it/s, loss=93.3]\n",
      "layer: 5: 100%|██████████| 1000/1000 [00:02<00:00, 349.99it/s, loss=46.3]\n",
      "layer: 6: 100%|██████████| 1000/1000 [00:02<00:00, 337.59it/s, loss=36.5]\n",
      "layer: 7: 100%|██████████| 1000/1000 [00:02<00:00, 335.25it/s, loss=8.36]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 407.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.007475920462929459\n",
      "[*] average ncut = 1.4897763014982055\n",
      "\n",
      "trial 8----------------------------\n",
      "> hidden dims = [187, 140, 105]\n",
      "> rho = 0.08105016126411585\n",
      "> beta = 75.10418138777538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 5000/5000 [00:14<00:00, 341.49it/s, loss=47]   \n",
      "layer: 1: 100%|██████████| 5000/5000 [00:14<00:00, 340.85it/s, loss=2.27]\n",
      "layer: 2: 100%|██████████| 5000/5000 [00:14<00:00, 342.87it/s, loss=0.287]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 392.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.008227392267905712\n",
      "[*] average ncut = 1.5129625666932238\n",
      "\n",
      "trial 9----------------------------\n",
      "> hidden dims = [200, 160, 128, 102, 81, 64, 51, 40, 32, 25, 20, 16, 12, 9, 7, 5, 4]\n",
      "> rho = 0.00018427970406864567\n",
      "> beta = 0.09548041810464164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 100/100 [00:00<00:00, 348.98it/s, loss=130]\n",
      "layer: 1: 100%|██████████| 100/100 [00:00<00:00, 350.63it/s, loss=3.29]\n",
      "layer: 2: 100%|██████████| 100/100 [00:00<00:00, 363.90it/s, loss=11.6]\n",
      "layer: 3: 100%|██████████| 100/100 [00:00<00:00, 353.08it/s, loss=4.89]\n",
      "layer: 4: 100%|██████████| 100/100 [00:00<00:00, 374.20it/s, loss=7.59]\n",
      "layer: 5: 100%|██████████| 100/100 [00:00<00:00, 336.02it/s, loss=5.81]\n",
      "layer: 6: 100%|██████████| 100/100 [00:00<00:00, 363.38it/s, loss=6.55]\n",
      "layer: 7: 100%|██████████| 100/100 [00:00<00:00, 371.69it/s, loss=6.75]\n",
      "layer: 8: 100%|██████████| 100/100 [00:00<00:00, 363.93it/s, loss=12.1]\n",
      "layer: 9: 100%|██████████| 100/100 [00:00<00:00, 382.28it/s, loss=27.5]\n",
      "layer: 10: 100%|██████████| 100/100 [00:00<00:00, 258.21it/s, loss=29.4]\n",
      "layer: 11: 100%|██████████| 100/100 [00:00<00:00, 327.40it/s, loss=24.6]\n",
      "layer: 12: 100%|██████████| 100/100 [00:00<00:00, 342.08it/s, loss=57.4]\n",
      "layer: 13: 100%|██████████| 100/100 [00:00<00:00, 337.26it/s, loss=72.9]\n",
      "layer: 14: 100%|██████████| 100/100 [00:00<00:00, 358.49it/s, loss=26.5]\n",
      "layer: 15: 100%|██████████| 100/100 [00:00<00:00, 327.67it/s, loss=32.2]\n",
      "layer: 16: 100%|██████████| 100/100 [00:00<00:00, 324.70it/s, loss=36.4]\n",
      "computing avg nmi and ncut:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] KMeans did not converge (not enough distinct points) --> Returning inf for avg_ncut\n",
      "\n",
      "trial 10----------------------------\n",
      "> hidden dims = [225, 202, 181, 162, 145, 130, 117, 105, 94, 84, 75, 67, 60, 54, 48, 43, 38, 34, 30, 27, 24, 21, 18, 16, 14, 12]\n",
      "> rho = 0.001276986535679652\n",
      "> beta = 0.012297288957910173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 1000/1000 [00:03<00:00, 331.99it/s, loss=51.2]\n",
      "layer: 1: 100%|██████████| 1000/1000 [00:02<00:00, 354.52it/s, loss=2.27]\n",
      "layer: 2: 100%|██████████| 1000/1000 [00:02<00:00, 337.46it/s, loss=1.89]\n",
      "layer: 3: 100%|██████████| 1000/1000 [00:02<00:00, 340.72it/s, loss=0.186]\n",
      "layer: 4: 100%|██████████| 1000/1000 [00:02<00:00, 346.34it/s, loss=1.41]\n",
      "layer: 5: 100%|██████████| 1000/1000 [00:03<00:00, 329.14it/s, loss=0.41]\n",
      "layer: 6: 100%|██████████| 1000/1000 [00:03<00:00, 329.85it/s, loss=1.24]\n",
      "layer: 7: 100%|██████████| 1000/1000 [00:02<00:00, 345.55it/s, loss=0.682]\n",
      "layer: 8: 100%|██████████| 1000/1000 [00:02<00:00, 340.45it/s, loss=1.35]\n",
      "layer: 9: 100%|██████████| 1000/1000 [00:02<00:00, 345.13it/s, loss=1.2]\n",
      "layer: 10: 100%|██████████| 1000/1000 [00:02<00:00, 348.95it/s, loss=1.47]\n",
      "layer: 11: 100%|██████████| 1000/1000 [00:03<00:00, 330.99it/s, loss=1.6]\n",
      "layer: 12: 100%|██████████| 1000/1000 [00:02<00:00, 335.08it/s, loss=1.41]\n",
      "layer: 13: 100%|██████████| 1000/1000 [00:02<00:00, 342.83it/s, loss=1.29]\n",
      "layer: 14: 100%|██████████| 1000/1000 [00:02<00:00, 341.07it/s, loss=1.17]\n",
      "layer: 15: 100%|██████████| 1000/1000 [00:02<00:00, 333.68it/s, loss=1.1]\n",
      "layer: 16: 100%|██████████| 1000/1000 [00:02<00:00, 343.27it/s, loss=1.04]\n",
      "layer: 17: 100%|██████████| 1000/1000 [00:02<00:00, 333.42it/s, loss=0.951]\n",
      "layer: 18: 100%|██████████| 1000/1000 [00:03<00:00, 333.10it/s, loss=0.871]\n",
      "layer: 19: 100%|██████████| 1000/1000 [00:02<00:00, 348.08it/s, loss=0.894]\n",
      "layer: 20: 100%|██████████| 1000/1000 [00:02<00:00, 339.28it/s, loss=0.649]\n",
      "layer: 21: 100%|██████████| 1000/1000 [00:03<00:00, 329.47it/s, loss=0.708]\n",
      "layer: 22: 100%|██████████| 1000/1000 [00:03<00:00, 325.79it/s, loss=0.85]\n",
      "layer: 23: 100%|██████████| 1000/1000 [00:03<00:00, 304.47it/s, loss=0.801]\n",
      "layer: 24: 100%|██████████| 1000/1000 [00:03<00:00, 316.23it/s, loss=0.789]\n",
      "layer: 25: 100%|██████████| 1000/1000 [00:03<00:00, 321.58it/s, loss=0.706]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 346.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.006546648262914634\n",
      "[*] average ncut = 1.2460477633118068\n",
      "\n",
      "trial 11----------------------------\n",
      "> hidden dims = [225, 202, 181, 162, 145, 130, 117, 105, 94, 84, 75, 67, 60, 54, 48, 43, 38, 34, 30, 27, 24, 21, 18, 16, 14, 12, 10, 9, 8, 7, 6, 5]\n",
      "> rho = 0.001194229221743439\n",
      "> beta = 0.010443437508657413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 1000/1000 [00:03<00:00, 274.67it/s, loss=50.7]\n",
      "layer: 1: 100%|██████████| 1000/1000 [00:03<00:00, 269.98it/s, loss=2.17]\n",
      "layer: 2: 100%|██████████| 1000/1000 [00:03<00:00, 274.59it/s, loss=1.52]\n",
      "layer: 3: 100%|██████████| 1000/1000 [00:03<00:00, 280.70it/s, loss=0.175]\n",
      "layer: 4: 100%|██████████| 1000/1000 [00:03<00:00, 302.43it/s, loss=1.29]\n",
      "layer: 5: 100%|██████████| 1000/1000 [00:03<00:00, 313.85it/s, loss=0.321]\n",
      "layer: 6: 100%|██████████| 1000/1000 [00:03<00:00, 281.44it/s, loss=1.11]\n",
      "layer: 7: 100%|██████████| 1000/1000 [00:02<00:00, 334.73it/s, loss=0.562]\n",
      "layer: 8: 100%|██████████| 1000/1000 [00:02<00:00, 345.11it/s, loss=1.3]\n",
      "layer: 9: 100%|██████████| 1000/1000 [00:03<00:00, 301.57it/s, loss=1.12]\n",
      "layer: 10: 100%|██████████| 1000/1000 [00:03<00:00, 313.63it/s, loss=1.46]\n",
      "layer: 11: 100%|██████████| 1000/1000 [00:02<00:00, 345.87it/s, loss=1.64]\n",
      "layer: 12: 100%|██████████| 1000/1000 [00:03<00:00, 290.04it/s, loss=1.56]\n",
      "layer: 13: 100%|██████████| 1000/1000 [00:03<00:00, 317.62it/s, loss=1.36]\n",
      "layer: 14: 100%|██████████| 1000/1000 [00:02<00:00, 342.25it/s, loss=1.01]\n",
      "layer: 15: 100%|██████████| 1000/1000 [00:03<00:00, 303.13it/s, loss=1]    \n",
      "layer: 16: 100%|██████████| 1000/1000 [00:03<00:00, 331.42it/s, loss=1.03]\n",
      "layer: 17: 100%|██████████| 1000/1000 [00:02<00:00, 337.97it/s, loss=0.939]\n",
      "layer: 18: 100%|██████████| 1000/1000 [00:03<00:00, 310.22it/s, loss=0.96]\n",
      "layer: 19: 100%|██████████| 1000/1000 [00:03<00:00, 291.48it/s, loss=0.704]\n",
      "layer: 20: 100%|██████████| 1000/1000 [00:03<00:00, 258.53it/s, loss=0.69]\n",
      "layer: 21: 100%|██████████| 1000/1000 [00:03<00:00, 262.34it/s, loss=0.704]\n",
      "layer: 22: 100%|██████████| 1000/1000 [00:03<00:00, 297.77it/s, loss=0.775]\n",
      "layer: 23: 100%|██████████| 1000/1000 [00:03<00:00, 299.99it/s, loss=0.822]\n",
      "layer: 24: 100%|██████████| 1000/1000 [00:03<00:00, 262.85it/s, loss=0.756]\n",
      "layer: 25: 100%|██████████| 1000/1000 [00:03<00:00, 282.72it/s, loss=1.02]\n",
      "layer: 26: 100%|██████████| 1000/1000 [00:03<00:00, 290.29it/s, loss=1]  \n",
      "layer: 27: 100%|██████████| 1000/1000 [00:03<00:00, 284.21it/s, loss=0.926]\n",
      "layer: 28: 100%|██████████| 1000/1000 [00:03<00:00, 278.92it/s, loss=1.2]\n",
      "layer: 29: 100%|██████████| 1000/1000 [00:03<00:00, 284.02it/s, loss=1.14]\n",
      "layer: 30: 100%|██████████| 1000/1000 [00:03<00:00, 300.02it/s, loss=1.31]\n",
      "layer: 31: 100%|██████████| 1000/1000 [00:03<00:00, 291.19it/s, loss=1.52]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 350.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.005343161470294304\n",
      "[*] average ncut = 1.2713821600461928\n",
      "\n",
      "trial 12----------------------------\n",
      "> hidden dims = [225, 202, 181, 162, 145, 130, 117, 105, 94, 84, 75, 67, 60, 54, 48, 43, 38, 34, 30, 27, 24, 21, 18, 16, 14, 12, 10, 9, 8, 7, 6, 5, 4, 3]\n",
      "> rho = 0.0016740028870066716\n",
      "> beta = 0.011302295738159825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 2500/2500 [00:08<00:00, 292.23it/s, loss=47.1]\n",
      "layer: 1: 100%|██████████| 2500/2500 [00:08<00:00, 297.76it/s, loss=6.96]\n",
      "layer: 2: 100%|██████████| 2500/2500 [00:08<00:00, 288.67it/s, loss=2.35]\n",
      "layer: 3: 100%|██████████| 2500/2500 [00:09<00:00, 263.80it/s, loss=1.7] \n",
      "layer: 4: 100%|██████████| 2500/2500 [00:08<00:00, 289.90it/s, loss=1.92]\n",
      "layer: 5: 100%|██████████| 2500/2500 [00:08<00:00, 296.41it/s, loss=1.72]\n",
      "layer: 6: 100%|██████████| 2500/2500 [00:09<00:00, 273.12it/s, loss=2.09] \n",
      "layer: 7: 100%|██████████| 2500/2500 [00:08<00:00, 286.29it/s, loss=2.82] \n",
      "layer: 8: 100%|██████████| 2500/2500 [00:08<00:00, 284.80it/s, loss=2.71] \n",
      "layer: 9: 100%|██████████| 2500/2500 [00:09<00:00, 263.95it/s, loss=3.25] \n",
      "layer: 10: 100%|██████████| 2500/2500 [00:10<00:00, 241.42it/s, loss=3.01] \n",
      "layer: 11: 100%|██████████| 2500/2500 [00:10<00:00, 232.57it/s, loss=3.36]\n",
      "layer: 12: 100%|██████████| 2500/2500 [00:11<00:00, 215.51it/s, loss=2.34]\n",
      "layer: 13: 100%|██████████| 2500/2500 [00:09<00:00, 261.32it/s, loss=2.13]\n",
      "layer: 14: 100%|██████████| 2500/2500 [00:10<00:00, 240.48it/s, loss=1.83]\n",
      "layer: 15: 100%|██████████| 2500/2500 [00:11<00:00, 210.85it/s, loss=1.56]\n",
      "layer: 16: 100%|██████████| 2500/2500 [00:11<00:00, 221.35it/s, loss=6.96]\n",
      "layer: 17: 100%|██████████| 2500/2500 [00:11<00:00, 218.97it/s, loss=13.7]\n",
      "layer: 18: 100%|██████████| 2500/2500 [00:10<00:00, 237.06it/s, loss=14.2]\n",
      "layer: 19: 100%|██████████| 2500/2500 [00:10<00:00, 234.93it/s, loss=10.9]\n",
      "layer: 20: 100%|██████████| 2500/2500 [00:10<00:00, 239.81it/s, loss=11.2]\n",
      "layer: 21: 100%|██████████| 2500/2500 [00:10<00:00, 237.77it/s, loss=11.2]\n",
      "layer: 22: 100%|██████████| 2500/2500 [00:10<00:00, 238.24it/s, loss=11.8]\n",
      "layer: 23: 100%|██████████| 2500/2500 [00:10<00:00, 238.95it/s, loss=8.4] \n",
      "layer: 24: 100%|██████████| 2500/2500 [00:10<00:00, 235.53it/s, loss=6.78]\n",
      "layer: 25: 100%|██████████| 2500/2500 [00:10<00:00, 237.10it/s, loss=6.87]\n",
      "layer: 26: 100%|██████████| 2500/2500 [00:10<00:00, 232.19it/s, loss=8.64]\n",
      "layer: 27: 100%|██████████| 2500/2500 [00:09<00:00, 250.78it/s, loss=3.89]\n",
      "layer: 28: 100%|██████████| 2500/2500 [00:10<00:00, 241.79it/s, loss=5.02]\n",
      "layer: 29: 100%|██████████| 2500/2500 [00:09<00:00, 251.76it/s, loss=4.85]\n",
      "layer: 30: 100%|██████████| 2500/2500 [00:10<00:00, 244.16it/s, loss=5.41]\n",
      "layer: 31: 100%|██████████| 2500/2500 [00:09<00:00, 250.09it/s, loss=6.27]\n",
      "layer: 32: 100%|██████████| 2500/2500 [00:09<00:00, 259.19it/s, loss=7.08]\n",
      "layer: 33: 100%|██████████| 2500/2500 [00:09<00:00, 259.04it/s, loss=6.83]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 364.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.020581842533657083\n",
      "[*] average ncut = 1.0797572981718313\n",
      "\n",
      "trial 13----------------------------\n",
      "> hidden dims = [225, 202, 181, 162, 145, 130, 117, 105, 94, 84, 75, 67, 60, 54, 48, 43, 38, 34, 30, 27, 24, 21, 18, 16, 14, 12, 10, 9, 8, 7, 6, 5, 4]\n",
      "> rho = 0.00180174544833436\n",
      "> beta = 0.0108259210062387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 2500/2500 [00:09<00:00, 270.82it/s, loss=44.7]\n",
      "layer: 1: 100%|██████████| 2500/2500 [00:07<00:00, 326.13it/s, loss=7.01]\n",
      "layer: 2: 100%|██████████| 2500/2500 [00:07<00:00, 326.37it/s, loss=2.6] \n",
      "layer: 3: 100%|██████████| 2500/2500 [00:07<00:00, 327.82it/s, loss=1.53]\n",
      "layer: 4: 100%|██████████| 2500/2500 [00:07<00:00, 330.31it/s, loss=1.84]\n",
      "layer: 5: 100%|██████████| 2500/2500 [00:08<00:00, 308.27it/s, loss=2.25] \n",
      "layer: 6: 100%|██████████| 2500/2500 [00:08<00:00, 300.95it/s, loss=2.64] \n",
      "layer: 7: 100%|██████████| 2500/2500 [00:07<00:00, 332.30it/s, loss=3.6]  \n",
      "layer: 8: 100%|██████████| 2500/2500 [00:07<00:00, 327.46it/s, loss=3.17] \n",
      "layer: 9: 100%|██████████| 2500/2500 [00:07<00:00, 318.42it/s, loss=3.64] \n",
      "layer: 10: 100%|██████████| 2500/2500 [00:08<00:00, 305.01it/s, loss=3.26] \n",
      "layer: 11: 100%|██████████| 2500/2500 [00:08<00:00, 289.57it/s, loss=2.65]\n",
      "layer: 12: 100%|██████████| 2500/2500 [00:08<00:00, 292.38it/s, loss=2.29]\n",
      "layer: 13: 100%|██████████| 2500/2500 [00:08<00:00, 308.25it/s, loss=1.99]\n",
      "layer: 14: 100%|██████████| 2500/2500 [00:08<00:00, 306.40it/s, loss=1.75]\n",
      "layer: 15: 100%|██████████| 2500/2500 [00:08<00:00, 283.75it/s, loss=12.3]\n",
      "layer: 16: 100%|██████████| 2500/2500 [00:08<00:00, 302.04it/s, loss=17.9]\n",
      "layer: 17: 100%|██████████| 2500/2500 [00:08<00:00, 297.34it/s, loss=14.3]\n",
      "layer: 18: 100%|██████████| 2500/2500 [00:08<00:00, 302.63it/s, loss=15.7]\n",
      "layer: 19: 100%|██████████| 2500/2500 [00:08<00:00, 294.52it/s, loss=12.1]\n",
      "layer: 20: 100%|██████████| 2500/2500 [00:07<00:00, 326.71it/s, loss=11.9]\n",
      "layer: 21: 100%|██████████| 2500/2500 [00:08<00:00, 287.70it/s, loss=11.2]\n",
      "layer: 22: 100%|██████████| 2500/2500 [00:07<00:00, 323.67it/s, loss=11.2]\n",
      "layer: 23: 100%|██████████| 2500/2500 [00:07<00:00, 330.35it/s, loss=6.74]\n",
      "layer: 24: 100%|██████████| 2500/2500 [00:07<00:00, 328.09it/s, loss=7.13]\n",
      "layer: 25: 100%|██████████| 2500/2500 [00:07<00:00, 323.07it/s, loss=7.8] \n",
      "layer: 26: 100%|██████████| 2500/2500 [00:07<00:00, 322.63it/s, loss=7.93]\n",
      "layer: 27: 100%|██████████| 2500/2500 [00:08<00:00, 308.20it/s, loss=3.71]\n",
      "layer: 28: 100%|██████████| 2500/2500 [00:08<00:00, 300.28it/s, loss=4.14]\n",
      "layer: 29: 100%|██████████| 2500/2500 [00:08<00:00, 306.24it/s, loss=3.99]\n",
      "layer: 30: 100%|██████████| 2500/2500 [00:07<00:00, 313.49it/s, loss=4.41]\n",
      "layer: 31: 100%|██████████| 2500/2500 [00:08<00:00, 289.86it/s, loss=4.41]\n",
      "layer: 32: 100%|██████████| 2500/2500 [00:08<00:00, 300.76it/s, loss=7.08]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 450.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.00773132391853962\n",
      "[*] average ncut = 1.0703472280039739\n",
      "\n",
      "trial 14----------------------------\n",
      "> hidden dims = [225, 202, 181, 162, 145, 130, 117, 105, 94, 84, 75, 67, 60, 54, 48, 43, 38, 34, 30, 27, 24, 21, 18, 16, 14, 12, 10, 9, 8, 7, 6, 5, 4, 3]\n",
      "> rho = 0.00234392752031698\n",
      "> beta = 0.5176025906267282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 2500/2500 [00:07<00:00, 312.60it/s, loss=62]  \n",
      "layer: 1: 100%|██████████| 2500/2500 [00:08<00:00, 308.29it/s, loss=17.7]\n",
      "layer: 2: 100%|██████████| 2500/2500 [00:07<00:00, 325.27it/s, loss=14.5]\n",
      "layer: 3: 100%|██████████| 2500/2500 [00:07<00:00, 322.36it/s, loss=12.5]\n",
      "layer: 4: 100%|██████████| 2500/2500 [00:07<00:00, 341.36it/s, loss=14.3] \n",
      "layer: 5: 100%|██████████| 2500/2500 [00:07<00:00, 349.41it/s, loss=16.9]  \n",
      "layer: 6: 100%|██████████| 2500/2500 [00:08<00:00, 305.01it/s, loss=19.5] \n",
      "layer: 7: 100%|██████████| 2500/2500 [00:08<00:00, 302.06it/s, loss=21.8] \n",
      "layer: 8: 100%|██████████| 2500/2500 [00:07<00:00, 316.27it/s, loss=24.1] \n",
      "layer: 9: 100%|██████████| 2500/2500 [00:08<00:00, 306.59it/s, loss=25.3]  \n",
      "layer: 10: 100%|██████████| 2500/2500 [00:07<00:00, 328.30it/s, loss=26.7]  \n",
      "layer: 11: 100%|██████████| 2500/2500 [00:07<00:00, 343.86it/s, loss=27.3] \n",
      "layer: 12: 100%|██████████| 2500/2500 [00:07<00:00, 345.43it/s, loss=31.4]\n",
      "layer: 13: 100%|██████████| 2500/2500 [00:07<00:00, 343.33it/s, loss=29.9]\n",
      "layer: 14: 100%|██████████| 2500/2500 [00:07<00:00, 345.68it/s, loss=29.8]\n",
      "layer: 15: 100%|██████████| 2500/2500 [00:07<00:00, 318.08it/s, loss=26]  \n",
      "layer: 16: 100%|██████████| 2500/2500 [00:08<00:00, 308.90it/s, loss=26.1]\n",
      "layer: 17: 100%|██████████| 2500/2500 [00:07<00:00, 317.29it/s, loss=22.3]\n",
      "layer: 18: 100%|██████████| 2500/2500 [00:07<00:00, 319.47it/s, loss=21.3]\n",
      "layer: 19: 100%|██████████| 2500/2500 [00:08<00:00, 301.76it/s, loss=17.7]\n",
      "layer: 20: 100%|██████████| 2500/2500 [00:08<00:00, 304.96it/s, loss=17.2]\n",
      "layer: 21: 100%|██████████| 2500/2500 [00:09<00:00, 276.69it/s, loss=15.7]\n",
      "layer: 22: 100%|██████████| 2500/2500 [00:08<00:00, 295.47it/s, loss=16.2]\n",
      "layer: 23: 100%|██████████| 2500/2500 [00:08<00:00, 309.98it/s, loss=11.2]\n",
      "layer: 24: 100%|██████████| 2500/2500 [00:07<00:00, 315.60it/s, loss=10.7]\n",
      "layer: 25: 100%|██████████| 2500/2500 [00:08<00:00, 290.87it/s, loss=11.3]\n",
      "layer: 26: 100%|██████████| 2500/2500 [00:09<00:00, 252.98it/s, loss=10.7]\n",
      "layer: 27: 100%|██████████| 2500/2500 [00:08<00:00, 298.21it/s, loss=6.31]\n",
      "layer: 28: 100%|██████████| 2500/2500 [00:08<00:00, 296.09it/s, loss=6.3] \n",
      "layer: 29: 100%|██████████| 2500/2500 [00:10<00:00, 244.09it/s, loss=6.55]\n",
      "layer: 30: 100%|██████████| 2500/2500 [00:09<00:00, 255.25it/s, loss=5.68]\n",
      "layer: 31: 100%|██████████| 2500/2500 [00:09<00:00, 256.59it/s, loss=5.61]\n",
      "layer: 32: 100%|██████████| 2500/2500 [00:09<00:00, 250.09it/s, loss=7.41]\n",
      "layer: 33: 100%|██████████| 2500/2500 [00:09<00:00, 260.48it/s, loss=6.64]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 380.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.01255871174497439\n",
      "[*] average ncut = 1.2991774485150323\n",
      "\n",
      "trial 15----------------------------\n",
      "> hidden dims = [212, 180, 153, 130, 110, 93, 79, 67, 56, 47, 39, 33, 28, 23, 19, 16, 13, 11, 9, 7, 5]\n",
      "> rho = 0.0028610388377873375\n",
      "> beta = 0.4496745823104299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 2500/2500 [00:08<00:00, 302.31it/s, loss=62.9]\n",
      "layer: 1: 100%|██████████| 2500/2500 [00:08<00:00, 296.59it/s, loss=19.5]\n",
      "layer: 2: 100%|██████████| 2500/2500 [00:07<00:00, 316.15it/s, loss=13.3]\n",
      "layer: 3: 100%|██████████| 2500/2500 [00:08<00:00, 299.70it/s, loss=8.95]\n",
      "layer: 4: 100%|██████████| 2500/2500 [00:07<00:00, 330.11it/s, loss=11.1]\n",
      "layer: 5: 100%|██████████| 2500/2500 [00:08<00:00, 302.04it/s, loss=12]   \n",
      "layer: 6: 100%|██████████| 2500/2500 [00:07<00:00, 332.69it/s, loss=15]   \n",
      "layer: 7: 100%|██████████| 2500/2500 [00:07<00:00, 322.25it/s, loss=17]   \n",
      "layer: 8: 100%|██████████| 2500/2500 [00:08<00:00, 303.55it/s, loss=17.6] \n",
      "layer: 9: 100%|██████████| 2500/2500 [00:08<00:00, 306.34it/s, loss=18.4]\n",
      "layer: 10: 100%|██████████| 2500/2500 [00:08<00:00, 291.96it/s, loss=24.9]\n",
      "layer: 11: 100%|██████████| 2500/2500 [00:08<00:00, 282.01it/s, loss=24.6]\n",
      "layer: 12: 100%|██████████| 2500/2500 [00:08<00:00, 282.39it/s, loss=22.2]\n",
      "layer: 13: 100%|██████████| 2500/2500 [00:09<00:00, 254.03it/s, loss=23.2]\n",
      "layer: 14: 100%|██████████| 2500/2500 [00:07<00:00, 314.76it/s, loss=19.3]\n",
      "layer: 15: 100%|██████████| 2500/2500 [00:09<00:00, 275.89it/s, loss=15.7]\n",
      "layer: 16: 100%|██████████| 2500/2500 [00:08<00:00, 291.12it/s, loss=14.8]\n",
      "layer: 17: 100%|██████████| 2500/2500 [00:07<00:00, 312.91it/s, loss=10.7]\n",
      "layer: 18: 100%|██████████| 2500/2500 [00:07<00:00, 323.56it/s, loss=9.89]\n",
      "layer: 19: 100%|██████████| 2500/2500 [00:08<00:00, 278.18it/s, loss=9.27]\n",
      "layer: 20: 100%|██████████| 2500/2500 [00:09<00:00, 274.87it/s, loss=12.2]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 424.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.007352642471465214\n",
      "[*] average ncut = 1.0265124688216316\n",
      "\n",
      "trial 16----------------------------\n",
      "> hidden dims = [212, 180, 153, 130, 110, 93, 79, 67, 56, 47, 39, 33, 28, 23, 19, 16, 13, 11]\n",
      "> rho = 0.03255881941776387\n",
      "> beta = 0.36405069134231305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 5000/5000 [00:17<00:00, 280.89it/s, loss=7.01]\n",
      "layer: 1: 100%|██████████| 5000/5000 [00:17<00:00, 292.48it/s, loss=7.1]   \n",
      "layer: 2: 100%|██████████| 5000/5000 [00:16<00:00, 301.11it/s, loss=10.2]  \n",
      "layer: 3: 100%|██████████| 5000/5000 [00:15<00:00, 316.25it/s, loss=14.5]  \n",
      "layer: 4: 100%|██████████| 5000/5000 [00:16<00:00, 295.54it/s, loss=18.1]  \n",
      "layer: 5: 100%|██████████| 5000/5000 [00:16<00:00, 303.14it/s, loss=23.7]  \n",
      "layer: 6: 100%|██████████| 5000/5000 [00:15<00:00, 322.70it/s, loss=33.1]  \n",
      "layer: 7: 100%|██████████| 5000/5000 [00:15<00:00, 326.62it/s, loss=42.3] \n",
      "layer: 8: 100%|██████████| 5000/5000 [00:15<00:00, 330.66it/s, loss=46]   \n",
      "layer: 9: 100%|██████████| 5000/5000 [00:15<00:00, 321.67it/s, loss=38.8]\n",
      "layer: 10: 100%|██████████| 5000/5000 [00:16<00:00, 294.34it/s, loss=29.9]\n",
      "layer: 11: 100%|██████████| 5000/5000 [00:17<00:00, 286.96it/s, loss=17.9]\n",
      "layer: 12: 100%|██████████| 5000/5000 [00:17<00:00, 282.04it/s, loss=12.2]\n",
      "layer: 13: 100%|██████████| 5000/5000 [00:17<00:00, 289.29it/s, loss=11.6]\n",
      "layer: 14: 100%|██████████| 5000/5000 [00:17<00:00, 289.24it/s, loss=10.1]\n",
      "layer: 15: 100%|██████████| 5000/5000 [00:17<00:00, 286.70it/s, loss=8.35]\n",
      "layer: 16: 100%|██████████| 5000/5000 [00:16<00:00, 294.47it/s, loss=7.18]\n",
      "layer: 17: 100%|██████████| 5000/5000 [00:15<00:00, 312.97it/s, loss=5.45]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 420.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.006439217810822169\n",
      "[*] average ncut = 1.3685896986048605\n",
      "\n",
      "trial 17----------------------------\n",
      "> hidden dims = [212, 180, 153, 130, 110, 93, 79, 67, 56, 47, 39, 33, 28, 23, 19, 16, 13, 11, 9]\n",
      "> rho = 0.0034335100607329173\n",
      "> beta = 0.6827687354505085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 2500/2500 [00:07<00:00, 319.09it/s, loss=66.4]\n",
      "layer: 1: 100%|██████████| 2500/2500 [00:07<00:00, 344.34it/s, loss=20.9]\n",
      "layer: 2: 100%|██████████| 2500/2500 [00:08<00:00, 295.23it/s, loss=16]  \n",
      "layer: 3: 100%|██████████| 2500/2500 [00:08<00:00, 294.39it/s, loss=10.4]\n",
      "layer: 4: 100%|██████████| 2500/2500 [00:08<00:00, 290.15it/s, loss=13.2]\n",
      "layer: 5: 100%|██████████| 2500/2500 [00:08<00:00, 311.54it/s, loss=13.7]\n",
      "layer: 6: 100%|██████████| 2500/2500 [00:08<00:00, 305.77it/s, loss=16.3] \n",
      "layer: 7: 100%|██████████| 2500/2500 [00:08<00:00, 292.06it/s, loss=18.6] \n",
      "layer: 8: 100%|██████████| 2500/2500 [00:08<00:00, 309.21it/s, loss=20.5] \n",
      "layer: 9: 100%|██████████| 2500/2500 [00:07<00:00, 321.85it/s, loss=23.4]\n",
      "layer: 10: 100%|██████████| 2500/2500 [00:07<00:00, 313.65it/s, loss=24.1]\n",
      "layer: 11: 100%|██████████| 2500/2500 [00:08<00:00, 309.97it/s, loss=27.5]\n",
      "layer: 12: 100%|██████████| 2500/2500 [00:07<00:00, 314.42it/s, loss=24.2]\n",
      "layer: 13: 100%|██████████| 2500/2500 [00:08<00:00, 311.47it/s, loss=23.9]\n",
      "layer: 14: 100%|██████████| 2500/2500 [00:08<00:00, 293.38it/s, loss=21.6]\n",
      "layer: 15: 100%|██████████| 2500/2500 [00:08<00:00, 278.99it/s, loss=18]  \n",
      "layer: 16: 100%|██████████| 2500/2500 [00:09<00:00, 270.88it/s, loss=15.7]\n",
      "layer: 17: 100%|██████████| 2500/2500 [00:08<00:00, 282.75it/s, loss=11.7]\n",
      "layer: 18: 100%|██████████| 2500/2500 [00:08<00:00, 286.02it/s, loss=10.7]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 403.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.017086920588152435\n",
      "[*] average ncut = 0.9799537492641504\n",
      "\n",
      "trial 18----------------------------\n",
      "> hidden dims = [212, 180, 153, 130, 110, 93, 79, 67, 56, 47, 39, 33, 28, 23, 19, 16, 13, 11]\n",
      "> rho = 0.004241127277176174\n",
      "> beta = 0.6687968305456375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 2500/2500 [00:08<00:00, 300.56it/s, loss=76.4]\n",
      "layer: 1: 100%|██████████| 2500/2500 [00:07<00:00, 321.09it/s, loss=20.8]\n",
      "layer: 2: 100%|██████████| 2500/2500 [00:07<00:00, 322.59it/s, loss=19.5]\n",
      "layer: 3: 100%|██████████| 2500/2500 [00:08<00:00, 310.98it/s, loss=6.25]\n",
      "layer: 4: 100%|██████████| 2500/2500 [00:08<00:00, 302.19it/s, loss=14.4]\n",
      "layer: 5: 100%|██████████| 2500/2500 [00:08<00:00, 301.31it/s, loss=6.79]\n",
      "layer: 6: 100%|██████████| 2500/2500 [00:08<00:00, 306.40it/s, loss=11.7]\n",
      "layer: 7: 100%|██████████| 2500/2500 [00:08<00:00, 294.97it/s, loss=7.46]\n",
      "layer: 8: 100%|██████████| 2500/2500 [00:07<00:00, 319.10it/s, loss=10.1]\n",
      "layer: 9: 100%|██████████| 2500/2500 [00:07<00:00, 312.86it/s, loss=8.45]\n",
      "layer: 10: 100%|██████████| 2500/2500 [00:07<00:00, 316.65it/s, loss=9.64]\n",
      "layer: 11: 100%|██████████| 2500/2500 [00:07<00:00, 315.54it/s, loss=8.94]\n",
      "layer: 12: 100%|██████████| 2500/2500 [00:08<00:00, 308.65it/s, loss=9.45]\n",
      "layer: 13: 100%|██████████| 2500/2500 [00:08<00:00, 285.27it/s, loss=8.57]\n",
      "layer: 14: 100%|██████████| 2500/2500 [00:08<00:00, 287.77it/s, loss=8.99]\n",
      "layer: 15: 100%|██████████| 2500/2500 [00:08<00:00, 289.65it/s, loss=8.71]\n",
      "layer: 16: 100%|██████████| 2500/2500 [00:08<00:00, 297.90it/s, loss=7.98]\n",
      "layer: 17: 100%|██████████| 2500/2500 [00:08<00:00, 310.90it/s, loss=7]   \n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 430.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.008342580867956393\n",
      "[*] average ncut = 1.574586618931203\n",
      "\n",
      "trial 19----------------------------\n",
      "> hidden dims = [200, 160, 128, 102, 81, 64, 51, 40, 32, 25, 20, 16, 12, 9, 7]\n",
      "> rho = 0.020766244840873534\n",
      "> beta = 1.7318945902290381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 100%|██████████| 5000/5000 [00:15<00:00, 313.19it/s, loss=57.4]\n",
      "layer: 1: 100%|██████████| 5000/5000 [00:16<00:00, 304.25it/s, loss=13.9]\n",
      "layer: 2: 100%|██████████| 5000/5000 [00:16<00:00, 304.14it/s, loss=11.1]\n",
      "layer: 3: 100%|██████████| 5000/5000 [00:16<00:00, 303.77it/s, loss=13.7]\n",
      "layer: 4: 100%|██████████| 5000/5000 [00:16<00:00, 307.24it/s, loss=17.3]  \n",
      "layer: 5: 100%|██████████| 5000/5000 [00:16<00:00, 296.15it/s, loss=21.6]  \n",
      "layer: 6: 100%|██████████| 5000/5000 [00:17<00:00, 283.64it/s, loss=26.6]  \n",
      "layer: 7: 100%|██████████| 5000/5000 [00:15<00:00, 323.94it/s, loss=33.6]\n",
      "layer: 8: 100%|██████████| 5000/5000 [00:15<00:00, 318.56it/s, loss=39.3]\n",
      "layer: 9: 100%|██████████| 5000/5000 [00:14<00:00, 342.73it/s, loss=35.3]\n",
      "layer: 10: 100%|██████████| 5000/5000 [00:14<00:00, 356.04it/s, loss=25]  \n",
      "layer: 11: 100%|██████████| 5000/5000 [00:16<00:00, 307.75it/s, loss=21.4]\n",
      "layer: 12: 100%|██████████| 5000/5000 [00:16<00:00, 308.33it/s, loss=20.6]\n",
      "layer: 13: 100%|██████████| 5000/5000 [00:14<00:00, 347.92it/s, loss=14.9]\n",
      "layer: 14: 100%|██████████| 5000/5000 [00:14<00:00, 343.17it/s, loss=12.5]\n",
      "computing avg nmi and ncut: 100%|██████████| 100/100 [00:00<00:00, 401.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] average nmi = 0.003824194187410867\n",
      "[*] average ncut = 1.103303976353766\n",
      "========================================================\n",
      "========================================================\n",
      "[*] best avg nmi = 0.020581842533657083\n",
      "[*] best avg ncut = 0.9799537492641504\n",
      "[*] best avg ncut avg nmi = 0.017086920588152435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Print trial number\n",
    "    print(f\"\\ntrial {trial.number}----------------------------\")\n",
    "    \n",
    "    # Set globals\n",
    "    global best_avg_nmi\n",
    "    global best_avg_ncut\n",
    "    global best_avg_ncut_avg_nmi\n",
    "    \n",
    "    # Set random seeds\n",
    "    torch.manual_seed(97)\n",
    "    np.random.seed(97)\n",
    "    random.seed(97)\n",
    "\n",
    "    # Suggest a decay rate for hidden dimensions\n",
    "    dim_decay_rate = trial.suggest_float(\"dim_decay_rate\", 0.6, 0.9, step=0.05)\n",
    "\n",
    "    # Compute the hidden dimensions\n",
    "    latent_dim = int(x_train.shape[1] * dim_decay_rate)\n",
    "    hidden_dims = []\n",
    "    hidden_dims.append(latent_dim)\n",
    "    while latent_dim * dim_decay_rate >= len(set(y)):\n",
    "        latent_dim = int(latent_dim * dim_decay_rate)\n",
    "        hidden_dims.append(latent_dim)\n",
    "\n",
    "    # Suggest the number of layers\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, len(hidden_dims), step=1)\n",
    "    hidden_dims = hidden_dims[:n_layers]\n",
    "    \n",
    "    # Create the model using the hidden dimensions\n",
    "    model = GraphEncoder(input_dim=x_train.shape[1], hidden_dims=hidden_dims).to(device)\n",
    "\n",
    "    # Suggest rho and beta for the sparsity constraint\n",
    "    rho = trial.suggest_float(\"rho\", 1e-4, 1e-1, log=True)\n",
    "    beta = trial.suggest_float(\"beta\", 1e-2, 1e3, log=True)\n",
    "    \n",
    "    # Suggest a learning rate for the optimizer and create the optimizer    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-3, 1e-2, log=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Create initial dataloader\n",
    "    current_x_train = x_train.clone().to(device)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(current_x_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    dataloader_iter = iter(dataloader)\n",
    "\n",
    "    # Suggest nb_epochs_per_layer\n",
    "    nb_epochs_per_layer = nb_epochs_per_layer_pool[trial.suggest_int(\"nb_epochs_per_layer\", 0, len(nb_epochs_per_layer_pool)-1)]\n",
    "    nb_train_iters = nb_epochs_per_layer * len(dataloader)\n",
    "\n",
    "    # Print some hyper parameters\n",
    "    print(\"> hidden dims =\", hidden_dims)\n",
    "    print(\"> rho =\", rho)\n",
    "    print(\"> beta =\", beta)\n",
    "    \n",
    "    # Launch the training loop\n",
    "    # For each layer in the stacked autoencoder: train the layer\n",
    "    for layer_number in range(len(model.autoencoders)):\n",
    "        for _ in (pb := tqdm.tqdm(range(nb_train_iters), desc=f\"layer: {layer_number}\")):\n",
    "            try:\n",
    "                (x_batch,) = next(dataloader_iter)\n",
    "            except StopIteration:\n",
    "                dataloader_iter = iter(dataloader)\n",
    "                (x_batch,) = next(dataloader_iter)\n",
    "            x_batch = x_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, decoded = model.autoencoders[layer_number](x_batch)\n",
    "            loss_1 = torch.nn.functional.mse_loss(decoded, x_batch, reduction='sum')\n",
    "            rho_hat = torch.mean(encoded, dim=0)\n",
    "            loss_2 = torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))\n",
    "            loss = loss_1 + beta * loss_2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pb.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        # Create new dataloader on the latent representations\n",
    "        with torch.no_grad():\n",
    "            current_x_train, _ = model.autoencoders[layer_number](current_x_train)\n",
    "            dataloader = torch.utils.data.DataLoader(\n",
    "                torch.utils.data.TensorDataset(current_x_train),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True\n",
    "            )\n",
    "            dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    try:\n",
    "        # Evaluate the model\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Get the encoded representations\n",
    "            encoded, _ = model(x_train)\n",
    "            encoded = encoded.to('cpu')\n",
    "            \n",
    "            # Evaluate average nmi and ncut over several kmeans runs\n",
    "            cumulated_nmi = 0\n",
    "            cumulated_ncut = 0\n",
    "            for _ in tqdm.tqdm(range(nb_kmeans_tests), desc=\"computing avg nmi and ncut\"):\n",
    "                kmeans = sklearn.cluster.KMeans(n_clusters=len(set(y)), algorithm=\"lloyd\", random_state=random.randint(0, 10000,), n_init='auto')\n",
    "                y_pred = kmeans.fit_predict(encoded)\n",
    "                cumulated_nmi += sklearn.metrics.normalized_mutual_info_score(y, y_pred)\n",
    "                cumulated_ncut += compute_ncut(s, y_pred)\n",
    "            avg_nmi = cumulated_nmi / nb_kmeans_tests\n",
    "            avg_ncut = cumulated_ncut / nb_kmeans_tests\n",
    "            \n",
    "            # Print average nmi and ncut\n",
    "            print(\"[*] average nmi =\", avg_nmi)\n",
    "            print(\"[*] average ncut =\", avg_ncut)\n",
    "            \n",
    "            # If average nmi is better than the best so far, update best_avg_nmi\n",
    "            if avg_nmi > best_avg_nmi:\n",
    "                best_avg_nmi = avg_nmi\n",
    "            \n",
    "            # If average ncut is better than the best so far, update best_avg_ncut and its corresponding average nmi (i.e. best_avg_ncut_avg_nmi)\n",
    "            if avg_ncut < best_avg_ncut:\n",
    "                best_avg_ncut = avg_ncut\n",
    "                best_avg_ncut_avg_nmi = avg_nmi\n",
    "    \n",
    "    except sklearn.exceptions.ConvergenceWarning:\n",
    "        print(\"[!] KMeans did not converge (not enough distinct points) --> Returning inf for avg_ncut\")\n",
    "        avg_ncut = float('inf')\n",
    "\n",
    "    # Return avg_ncut as the objective to minimize\n",
    "    return avg_ncut\n",
    "\n",
    "\n",
    "# Set global parameters\n",
    "nb_epochs_per_layer_pool = [10, 100, 500, 1000, 2500, 5000]\n",
    "nb_kmeans_tests = 100\n",
    "nb_trials = 20\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu'); print(\"[*] using device:\", device)\n",
    "x_train = torch.tensor(nts, dtype=torch.float32).to(device)\n",
    "batch_size = x_train.shape[0]\n",
    "\n",
    "# Set globals to track best results\n",
    "best_avg_nmi = 0.0\n",
    "best_avg_ncut = float('inf')\n",
    "best_avg_ncut_avg_nmi = 0.0\n",
    "\n",
    "# Run optuna study\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(sampler=sampler, direction=\"minimize\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study.optimize(objective, n_trials=nb_trials)\n",
    "\n",
    "# Display the best results\n",
    "print(\"========================================================\")\n",
    "print(\"========================================================\")\n",
    "print(\"[*] best avg nmi =\", best_avg_nmi)\n",
    "print(\"[*] best avg ncut =\", best_avg_ncut)\n",
    "print(\"[*] best avg ncut avg nmi =\", best_avg_ncut_avg_nmi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
