{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e580452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = torch.nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.sigmoid(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c200d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.autoencoders = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.autoencoders.append(AutoEncoder(prev_dim, hidden_dim))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "    def forward(self, X, train_mode, **kwargs):\n",
    "        if train_mode == 'layerwise':\n",
    "            layer_number = kwargs.get('layer_number', None)\n",
    "            if layer_number is None or layer_number < 0 or layer_number >= len(self.autoencoders):\n",
    "                raise ValueError(\"Invalid layer number for layerwise training\")\n",
    "            encoded = torch.sigmoid(self.autoencoders[layer_number].encoder(X))\n",
    "            decoded = torch.sigmoid(self.autoencoders[layer_number].decoder(encoded))\n",
    "            return encoded, decoded\n",
    "            \n",
    "        elif train_mode == 'endtoend':\n",
    "            for autoencoder in self.autoencoders:\n",
    "                X = torch.sigmoid(autoencoder.encoder(X))\n",
    "            encoded = X\n",
    "            for autoencoder in reversed(self.autoencoders):\n",
    "                X = torch.sigmoid(autoencoder.decoder(X))\n",
    "            decoded = X\n",
    "            return encoded, decoded\n",
    "    \n",
    "    def train(self,\n",
    "              X,\n",
    "              compile,\n",
    "              train_mode,\n",
    "              iters,\n",
    "              optimizer,\n",
    "              rho=0.01,\n",
    "              beta=1.0,\n",
    "              batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = X.shape[0] - 1\n",
    "\n",
    "        if compile==\"True\":\n",
    "            train_model = torch.compile(self)\n",
    "        else:\n",
    "            train_model = self\n",
    "        \n",
    "        if train_mode == 'layerwise':\n",
    "            for layer_number in range(len(self.autoencoders)):\n",
    "                for _ in tqdm.tqdm(range(iters), desc=f\"Training layer {layer_number}\"):\n",
    "                    batch_idx = torch.randint(0, X.shape[0]-batch_size, (1,)).item()\n",
    "                    X_batch = X[batch_idx : batch_idx + batch_size]\n",
    "                    optimizer.zero_grad()\n",
    "                    encoded, decoded = train_model(X_batch, train_mode='layerwise', layer_number=layer_number)\n",
    "                    loss_1 = torch.nn.functional.mse_loss(decoded, X_batch, reduction='sum')\n",
    "                    rho_hat = torch.mean(encoded, dim=0)\n",
    "                    loss_2 = torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))\n",
    "                    loss = loss_1 + beta * loss_2\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                X = torch.sigmoid(self.autoencoders[layer_number].encoder(X)).detach()\n",
    "        elif train_mode == 'endtoend':\n",
    "            for _ in tqdm.tqdm(range(iters)):\n",
    "                batch_idx = torch.randint(0, X.shape[0]-batch_size, (1,)).item()\n",
    "                X_batch = X[batch_idx : batch_idx + batch_size]\n",
    "                optimizer.zero_grad()\n",
    "                encoded, decoded = train_model(X_batch, train_mode='endtoend')\n",
    "                loss_1 = torch.nn.functional.mse_loss(decoded, X_batch, reduction='sum')\n",
    "                rho_hat = torch.mean(encoded, dim=0)\n",
    "                loss_2 = torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))\n",
    "                loss = loss_1 + beta * loss_2\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode(self, X):\n",
    "        for autoencoder in self.autoencoders:\n",
    "            X = torch.sigmoid(autoencoder.encoder(X))\n",
    "        return X\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode_decode(self, X):\n",
    "        for autoencoder in self.autoencoders:\n",
    "            X = torch.sigmoid(autoencoder.encoder(X))\n",
    "        for autoencoder in reversed(self.autoencoders):\n",
    "            X = torch.sigmoid(autoencoder.decoder(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.load_digits(return_X_y=True, as_frame=False)\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NB_EPOCHS = 5000\n",
    "BATCH_SIZE = 256\n",
    "X = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "graph_encoder = GraphEncoder(input_dim=X.shape[1], hidden_dims=[32, 16, 8]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(graph_encoder.parameters(), lr=0.01)\n",
    "nb_iters = NB_EPOCHS * (X.shape[0] // BATCH_SIZE)\n",
    "graph_encoder.train(X, compile=\"compile\", train_mode=\"endtoend\", iters=nb_iters, optimizer=optimizer, rho=0.01, beta=1.0, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54636c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xh = graph_encoder.encode_decode(X).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.randint(0, X.shape[0], (1,)).item()\n",
    "plt.imshow(X[i].to(\"cpu\").reshape(8, 8), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(Xh[i].to(\"cpu\").reshape(8, 8), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
