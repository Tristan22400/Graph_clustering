{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d1d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younes/miniconda3/envs/ai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "import random\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bea35",
   "metadata": {},
   "source": [
    "# 1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e580452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = torch.nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.sigmoid(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c200d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.autoencoders = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.autoencoders.append(AutoEncoder(prev_dim, hidden_dim))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        for autoencoder in self.autoencoders:\n",
    "            x = torch.sigmoid(autoencoder.encoder(x))\n",
    "        encoded = x\n",
    "        for autoencoder in reversed(self.autoencoders):\n",
    "            x = torch.sigmoid(autoencoder.decoder(x))\n",
    "        decoded = x\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0bc1a",
   "metadata": {},
   "source": [
    "# 2. Test on benchmark \"email\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c57755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ncut(s, labels):\n",
    "    \"\"\"\n",
    "    Compute  normalized cut for given similarity matrix s and cluster labels:\n",
    "      Ncut = sum_k cut(C_k, V\\C_k) / assoc(C_k, V)\n",
    "    where\n",
    "      cut(C, V\\C) = sum_{i in C, j not in C} A[i,j]\n",
    "      assoc(C, V) = sum_{i in C, j in V} A[i,j]  (i.e., volume of C)\n",
    "    A : symmetric adjacency/similarity numpy array\n",
    "    labels : length-n array of integer cluster labels\n",
    "    Returns float Ncut value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the unique labels in the community assignment\n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    # Precompute degrees\n",
    "    degrees = s.sum(axis=1)  # degree/volume per node\n",
    "    \n",
    "    # Initialize ncut\n",
    "    ncut = 0.0\n",
    "    \n",
    "    # For each cluster compute link and volume, then sum up to get ncut\n",
    "    for lab in unique_labels:\n",
    "        \n",
    "        # Get the indices of nodes in cluster lab\n",
    "        idx = np.where(labels == lab)[0]\n",
    "        if idx.size == 0:\n",
    "            raise Exception(\"compute_ncut_from_labels: empty cluster found in labels.\")\n",
    "        \n",
    "        # Compute volume = sum of degrees of nodes in idx\n",
    "        volume = degrees[idx].sum()\n",
    "        \n",
    "        # If volume is not zero, compute link to get the local cut then sum to ncut, otherwise skip (i.e. cut = 0)\n",
    "        if volume != 0:\n",
    "\n",
    "            # Compute link = sum over i in C, j not in C, of A[i,j]\n",
    "            # = volume - internal connections\n",
    "            internal_connections = s[np.ix_(idx, idx)].sum()\n",
    "            link = volume - internal_connections\n",
    "            \n",
    "            # Compute local cut contribution\n",
    "            local_cut = link / volume\n",
    "\n",
    "            # Sum to ncut\n",
    "            ncut += local_cut\n",
    "    \n",
    "    return ncut\n",
    "\n",
    "warnings.filterwarnings(\"error\", category=sklearn.exceptions.ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Graph is not fully connected, spectral embedding may not work as expected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c26a6",
   "metadata": {},
   "source": [
    "## 2.1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2468289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nts.shape: (1005, 1005)\n",
      "[*] number of clusters: 42\n",
      "[*] nmi: 0.40867148927590957\n",
      "[*] ncut: 11.101097091130402\n"
     ]
    }
   ],
   "source": [
    "nxg = nx.read_gml(\"../datasets/real/email/email.gml\") # read the football gml file into a networkx graph\n",
    "y = [nxg.nodes[n][\"value\"] for n in nxg.nodes] # extract the ground-truth community labels\n",
    "s = nx.to_numpy_array(nxg) # generate the similarity matrix\n",
    "s = s + np.diag(np.ones(nxg.number_of_nodes())) # we add self-loops (not indicated in the original paper but improves performance)\n",
    "nts = s / np.sum(s, axis=1, keepdims=True) # generate the normalized training set\n",
    "print(\"[*] nts.shape:\", nts.shape)\n",
    "print(\"[*] number of clusters:\", len(set(y)))\n",
    "y_pred = sklearn.cluster.KMeans(n_clusters=len(set(y)), n_init=100, random_state=97).fit_predict(nts)\n",
    "nmi = sklearn.metrics.normalized_mutual_info_score(y, y_pred)\n",
    "ncut = compute_ncut(nts, y_pred)\n",
    "print(\"[*] nmi:\", nmi)\n",
    "print(\"[*] ncut:\", ncut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09edc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi: 0.487196951731793\n",
      "[*] ncut: 20.625335338205108\n"
     ]
    }
   ],
   "source": [
    "y_pred = sklearn.cluster.SpectralClustering(n_clusters=len(set(y)), affinity='precomputed', assign_labels='kmeans', n_init=100, random_state=97,).fit_predict(s)\n",
    "nmi = sklearn.metrics.normalized_mutual_info_score(y, y_pred)\n",
    "ncut = compute_ncut(nts, y_pred)\n",
    "print(\"[*] nmi:\", nmi)\n",
    "print(\"[*] ncut:\", ncut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5901e2a",
   "metadata": {},
   "source": [
    "## 2.3. Model training with hyper-parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-01 13:01:06,016] A new study created in memory with name: no-name-47730f95-8094-493c-9533-d456c5b48ec5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] using device: cuda\n",
      "\n",
      "trial 0----------------------------\n",
      "> hidden dims = [703, 492, 344, 240, 168, 117, 81, 56]\n",
      "> rho = 0.015702970884055395\n",
      "> beta = 9.846738873614559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 23900it [03:00, 132.78it/s, loss=0.237, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 0 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 1: 21608it [02:22, 151.75it/s, loss=0.729, stab=4] \n",
      "layer: 2: 23093it [02:28, 155.77it/s, loss=0.961, stab=4] \n",
      "layer: 3: 24579it [02:39, 154.06it/s, loss=2.6, stab=4]   \n",
      "layer: 4: 26450it [02:53, 152.56it/s, loss=5.06, stab=4]  \n",
      "layer: 5: 27590it [03:00, 153.28it/s, loss=8.34, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 5 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 6: 27899it [03:00, 154.99it/s, loss=11.6, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 6 training after 180.01s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 7: 27695it [03:00, 153.86it/s, loss=17.6, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 7 training after 180.01s (> 180s)\n",
      "[*] nmi = 0.10900818828395596\n",
      "[*] ncut = 32.548597640405525\n",
      "\n",
      "trial 1----------------------------\n",
      "> hidden dims = [653]\n",
      "> rho = 0.0396760507705299\n",
      "> beta = 10.129197956845726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 17135it [02:08, 133.35it/s, loss=0.366, stab=4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.6659256695667087\n",
      "[*] ncut = 21.78386129964417\n",
      "\n",
      "trial 2----------------------------\n",
      "> hidden dims = [603, 361, 216, 129, 77, 46]\n",
      "> rho = 0.03142880890840111\n",
      "> beta = 0.1152644954031561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 22497it [02:32, 147.62it/s, loss=0.18, stab=4] \n",
      "layer: 1: 22730it [02:25, 156.39it/s, loss=4.67, stab=4]  \n",
      "layer: 2: 28281it [02:59, 157.12it/s, loss=155, stab=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 2 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 3: 28305it [03:00, 157.25it/s, loss=1.97e+3, stab=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 3 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 4: 28413it [02:57, 159.82it/s, loss=800, stab=4]   \n",
      "layer: 5: 12470it [01:17, 160.79it/s, loss=312, stab=4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.4725404216672759\n",
      "[*] ncut = 27.262347330186337\n",
      "\n",
      "trial 3----------------------------\n",
      "> hidden dims = [653, 424, 275]\n",
      "> rho = 0.0037520558551242854\n",
      "> beta = 1.4445251022763053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 26534it [03:00, 147.41it/s, loss=1.49, stab=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 0 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 1: 24180it [02:14, 179.50it/s, loss=0.39, stab=4]  \n",
      "layer: 2: 25776it [02:20, 183.44it/s, loss=0.445, stab=4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.22437918184212768\n",
      "[*] ncut = 29.912024298008987\n",
      "\n",
      "trial 4----------------------------\n",
      "> hidden dims = [804, 643]\n",
      "> rho = 0.0007523742884534858\n",
      "> beta = 0.6789053271698483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 26915it [02:52, 156.04it/s, loss=1.64, stab=4]\n",
      "layer: 1: 22715it [02:17, 165.58it/s, loss=0.886, stab=4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.26678221889744097\n",
      "[*] ncut = 22.453191262176478\n",
      "\n",
      "trial 5----------------------------\n",
      "> hidden dims = [854, 725, 616, 523]\n",
      "> rho = 0.003489018845491387\n",
      "> beta = 9.163741808778772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 27510it [03:00, 152.83it/s, loss=0.966, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 0 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 1: 25085it [02:37, 159.26it/s, loss=0.427, stab=4] \n",
      "layer: 2: 25038it [02:26, 170.75it/s, loss=0.377, stab=4] \n",
      "layer: 3: 24585it [02:20, 174.50it/s, loss=0.341, stab=4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.16572609312672865\n",
      "[*] ncut = 28.52338374792191\n",
      "\n",
      "trial 6----------------------------\n",
      "> hidden dims = [804, 643, 514]\n",
      "> rho = 0.00015673095467235422\n",
      "> beta = 555.1721685244722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 9980it [01:05, 153.33it/s, loss=132, stab=4]    \n",
      "layer: 1: 11638it [01:11, 163.86it/s, loss=0.2, stab=4]   \n",
      "layer: 2: 11096it [01:04, 171.92it/s, loss=0.221, stab=4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.23024387223367754\n",
      "[*] ncut = 27.42581561676402\n",
      "\n",
      "trial 7----------------------------\n",
      "> hidden dims = [854, 725, 616, 523, 444, 377]\n",
      "> rho = 0.00019634341572933326\n",
      "> beta = 26.373339933815235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 28880it [03:00, 160.44it/s, loss=37.2, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 0 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 1: 18203it [01:50, 164.90it/s, loss=65, stab=4]    \n",
      "layer: 2: 15968it [01:31, 175.19it/s, loss=0.443, stab=4] \n",
      "layer: 3: 15892it [01:28, 179.67it/s, loss=0.736, stab=4] \n",
      "layer: 4: 15409it [01:24, 182.30it/s, loss=0.51, stab=4]  \n",
      "layer: 5: 15436it [01:23, 184.37it/s, loss=0.366, stab=4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.18520481559803229\n",
      "[*] ncut = 27.981113420424478\n",
      "\n",
      "trial 8----------------------------\n",
      "> hidden dims = [603, 361, 216]\n",
      "> rho = 0.00012681352169084607\n",
      "> beta = 352.0481045526035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 14756it [01:28, 166.71it/s, loss=132, stab=4]   \n",
      "layer: 1: 16894it [01:32, 181.84it/s, loss=0.16, stab=4]  \n",
      "layer: 2: 16792it [01:31, 183.59it/s, loss=0.128, stab=4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.2246669352003012\n",
      "[*] ncut = 26.50421795145601\n",
      "\n",
      "trial 9----------------------------\n",
      "> hidden dims = [804, 643, 514, 411, 328]\n",
      "> rho = 0.0036324869566766076\n",
      "> beta = 5.414413211338521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 28777it [03:00, 159.87it/s, loss=0.349, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 0 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 1: 21888it [02:09, 169.13it/s, loss=0.482, stab=4] \n",
      "layer: 2: 23127it [02:10, 177.20it/s, loss=0.277, stab=4] \n",
      "layer: 3: 24255it [02:14, 180.75it/s, loss=0.3, stab=4]   \n",
      "layer: 4: 23769it [02:12, 179.38it/s, loss=0.654, stab=4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.1817473453929935\n",
      "[*] ncut = 31.324465992714174\n",
      "\n",
      "trial 10----------------------------\n",
      "> hidden dims = [703]\n",
      "> rho = 0.08102356207766644\n",
      "> beta = 0.012297288957910173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 54it [00:00, 119.21it/s, loss=138, stab=4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.26668177924640746\n",
      "[*] ncut = 10.816101219789385\n",
      "\n",
      "trial 11----------------------------\n",
      "> hidden dims = [703]\n",
      "> rho = 0.0869821884209373\n",
      "> beta = 0.0267870779847426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 57it [00:00, 166.24it/s, loss=142, stab=4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.41438994582445154\n",
      "[*] ncut = 10.22166811805229\n",
      "\n",
      "trial 12----------------------------\n",
      "> hidden dims = [703]\n",
      "> rho = 0.08332447280612446\n",
      "> beta = 0.014747073255776684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 56it [00:00, 157.36it/s, loss=138, stab=4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.39824210663672405\n",
      "[*] ncut = 10.399832664811305\n",
      "\n",
      "trial 13----------------------------\n",
      "> hidden dims = [703]\n",
      "> rho = 0.0987188295669722\n",
      "> beta = 0.01094257583586978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 64it [00:00, 167.65it/s, loss=137, stab=4]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.28370623558085745\n",
      "[*] ncut = 11.292569790447335\n",
      "\n",
      "trial 14----------------------------\n",
      "> hidden dims = [753, 564, 423, 317, 237, 177, 132, 99, 74]\n",
      "> rho = 0.011710530134441607\n",
      "> beta = 0.07923398067978257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 19045it [02:01, 157.38it/s, loss=0.5, stab=4]  \n",
      "layer: 1: 10073it [01:00, 167.85it/s, loss=9.31, stab=4]  \n",
      "layer: 2: 31125it [03:00, 172.91it/s, loss=38.6, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 2 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 3: 31670it [03:00, 175.94it/s, loss=256, stab=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 3 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 4: 31722it [03:00, 176.22it/s, loss=904, stab=0]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 4 training after 180.01s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 5: 31684it [03:00, 176.02it/s, loss=949, stab=0]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 5 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 6: 31714it [03:00, 176.19it/s, loss=557, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 6 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 7: 31918it [03:00, 177.32it/s, loss=339, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 7 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 8: 12702it [01:11, 177.36it/s, loss=238, stab=4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.36557914276460624\n",
      "[*] ncut = 30.26994150476641\n",
      "\n",
      "trial 15----------------------------\n",
      "> hidden dims = [753, 564]\n",
      "> rho = 0.012591180377658155\n",
      "> beta = 0.08662897127781095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 17435it [01:50, 157.59it/s, loss=0.585, stab=4]\n",
      "layer: 1: 30859it [03:00, 171.44it/s, loss=0.438, stab=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 1 training after 180.00s (> 180s)\n",
      "[*] nmi = 0.45009339528211467\n",
      "[*] ncut = 19.29291800338486\n",
      "\n",
      "trial 16----------------------------\n",
      "> hidden dims = [904, 813, 731, 657, 591, 531, 477, 429, 386, 347, 312, 280, 252, 226, 203, 182, 163, 146, 131, 117, 105, 94, 84, 75]\n",
      "> rho = 0.03936865284316899\n",
      "> beta = 0.29422792524628416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 15854it [01:44, 151.80it/s, loss=0.262, stab=4]\n",
      "layer: 1: 23283it [02:32, 152.26it/s, loss=0.334, stab=4]\n",
      "layer: 2: 28392it [03:00, 157.73it/s, loss=2.1, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 2 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 3: 28983it [03:00, 161.01it/s, loss=2.22, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 3 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 4: 31040it [03:00, 172.44it/s, loss=4.3, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 4 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 5: 31499it [03:00, 174.99it/s, loss=7.85, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 5 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 6: 31752it [03:00, 176.40it/s, loss=18.3, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 6 training after 180.01s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 7: 32223it [03:00, 179.01it/s, loss=52.2, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 7 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 8: 32221it [03:00, 179.00it/s, loss=129, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 8 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 9: 32280it [03:00, 179.33it/s, loss=241, stab=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 9 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 10: 32658it [03:00, 181.43it/s, loss=368, stab=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 10 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 11: 32638it [03:00, 181.32it/s, loss=408, stab=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 11 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 12: 32686it [03:00, 181.59it/s, loss=423, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 12 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 13: 32252it [03:00, 179.17it/s, loss=368, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 13 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 14: 31931it [03:00, 177.39it/s, loss=293, stab=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 14 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 15: 32148it [03:00, 178.60it/s, loss=227, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 15 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 16: 32332it [03:00, 179.62it/s, loss=200, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 16 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 17: 32149it [03:00, 178.60it/s, loss=155, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 17 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 18: 30919it [03:00, 171.77it/s, loss=119, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 18 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 19: 31076it [03:00, 172.64it/s, loss=85.6, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 19 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 20: 30925it [03:00, 171.80it/s, loss=63.3, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 20 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 21: 31084it [03:00, 172.69it/s, loss=51.8, stab=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 21 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 22: 31984it [03:00, 177.69it/s, loss=41.7, stab=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 22 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 23: 5446it [00:30, 180.47it/s, loss=30.1, stab=4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.20354354721160833\n",
      "[*] ncut = 34.30373032351829\n",
      "\n",
      "trial 17----------------------------\n",
      "> hidden dims = [653, 424]\n",
      "> rho = 0.0008134650578175098\n",
      "> beta = 0.027553913125744383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 54it [00:00, 164.41it/s, loss=147, stab=4]  \n",
      "layer: 1: 134it [00:00, 159.90it/s, loss=1.92, stab=4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.21648643398318013\n",
      "[*] ncut = 23.68756028831647\n",
      "\n",
      "trial 18----------------------------\n",
      "> hidden dims = [703]\n",
      "> rho = 0.022984705474655778\n",
      "> beta = 0.04022710905341625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 70it [00:00, 125.84it/s, loss=151, stab=4]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] nmi = 0.38411973586187215\n",
      "[*] ncut = 11.113397719223542\n",
      "\n",
      "trial 19----------------------------\n",
      "> hidden dims = [753, 564, 423, 317, 237, 177, 132, 99]\n",
      "> rho = 0.08087411777442799\n",
      "> beta = 0.2207543157923934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 0: 15792it [01:43, 152.46it/s, loss=0.198, stab=4]\n",
      "layer: 1: 30294it [03:00, 168.30it/s, loss=8.25, stab=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 1 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 2: 31054it [03:00, 172.52it/s, loss=45.9, stab=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] stopping layer 2 training after 180.00s (> 180s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "layer: 3: 9266it [00:58, 168.30it/s, loss=478, stab=0]    "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Print trial number\n",
    "    print(f\"\\ntrial {trial.number}----------------------------\")\n",
    "    \n",
    "    # Set globals\n",
    "    global best_nmi\n",
    "    global best_ncut\n",
    "    global best_ncut_nmi\n",
    "    global loss_tolerance\n",
    "    global stab_tolerance\n",
    "    global max_time_per_layer\n",
    "    \n",
    "    # Set random seeds\n",
    "    torch.manual_seed(97)\n",
    "    np.random.seed(97)\n",
    "    random.seed(97)\n",
    "\n",
    "    # Suggest a decay rate for hidden dimensions\n",
    "    dim_decay_rate = trial.suggest_float(\"dim_decay_rate\", 0.6, 0.9, step=0.05)\n",
    "\n",
    "    # Compute the hidden dimensions\n",
    "    latent_dim = int(x_train.shape[1] * dim_decay_rate)\n",
    "    hidden_dims = []\n",
    "    hidden_dims.append(latent_dim)\n",
    "    while latent_dim * dim_decay_rate >= len(set(y)):\n",
    "        latent_dim = int(latent_dim * dim_decay_rate)\n",
    "        hidden_dims.append(latent_dim)\n",
    "\n",
    "    # Suggest the number of layers\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, len(hidden_dims), step=1)\n",
    "    hidden_dims = hidden_dims[:n_layers]\n",
    "    \n",
    "    # Create the model using the hidden dimensions\n",
    "    model = GraphEncoder(input_dim=x_train.shape[1], hidden_dims=hidden_dims).to(device)\n",
    "\n",
    "    # Suggest rho and beta for the sparsity constraint\n",
    "    rho = trial.suggest_float(\"rho\", 1e-4, 1e-1, log=True)\n",
    "    beta = trial.suggest_float(\"beta\", 1e-2, 1e3, log=True)\n",
    "    \n",
    "    # Suggest a learning rate for the optimizer and create the optimizer    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-3, 1e-2, log=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Create initial dataloader\n",
    "    current_x_train = x_train.clone().to(device)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(current_x_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    dataloader_iter = iter(dataloader)\n",
    "\n",
    "    # Suggest nb_epochs_per_layer\n",
    "    # nb_epochs_per_layer = nb_epochs_per_layer_pool[trial.suggest_int(\"nb_epochs_per_layer\", 0, len(nb_epochs_per_layer_pool)-1)]\n",
    "    # nb_train_iters = nb_epochs_per_layer * len(dataloader)\n",
    "\n",
    "    # Print some hyper parameters\n",
    "    print(\"> hidden dims =\", hidden_dims)\n",
    "    print(\"> rho =\", rho)\n",
    "    print(\"> beta =\", beta)\n",
    "    \n",
    "    # Launch the training loop\n",
    "    # For each layer in the stacked autoencoder: train the layer\n",
    "    for layer_number in range(len(model.autoencoders)):\n",
    "        stop = False\n",
    "        last_loss = None\n",
    "        start_time = time.time()\n",
    "        pb = tqdm.tqdm(desc=f\"layer: {layer_number}\")\n",
    "        stab = 0\n",
    "        while not stop:\n",
    "            try:\n",
    "                (x_batch,) = next(dataloader_iter)\n",
    "            except StopIteration:\n",
    "                dataloader_iter = iter(dataloader)\n",
    "                (x_batch,) = next(dataloader_iter)\n",
    "            x_batch = x_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, decoded = model.autoencoders[layer_number](x_batch)\n",
    "            loss_1 = torch.nn.functional.mse_loss(decoded, x_batch, reduction='sum')\n",
    "            rho_hat = torch.mean(encoded, dim=0)\n",
    "            loss_2 = torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))\n",
    "            loss = loss_1 + beta * loss_2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Stop criteria\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > max_time_per_layer:\n",
    "                print(f\"[!] stopping layer {layer_number} training after {elapsed_time:.2f}s (> {max_time_per_layer}s)\")\n",
    "                pb.close()\n",
    "                break\n",
    "            if last_loss is None:\n",
    "                last_loss = loss.item()\n",
    "            else:\n",
    "                if abs(last_loss - loss.item()) < loss_tolerance:\n",
    "                    stab += 1\n",
    "                    if stab == stab_tolerance:\n",
    "                        stop = True\n",
    "                        pb.close()\n",
    "                else:\n",
    "                    stab = 0\n",
    "                last_loss = loss.item()\n",
    "            pb.set_postfix({\"loss\": loss.item(), \"stab\": stab})\n",
    "            pb.update(1)\n",
    "\n",
    "        # Create new dataloader on the latent representations\n",
    "        with torch.no_grad():\n",
    "            current_x_train, _ = model.autoencoders[layer_number](current_x_train)\n",
    "            dataloader = torch.utils.data.DataLoader(\n",
    "                torch.utils.data.TensorDataset(current_x_train),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True\n",
    "            )\n",
    "            dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    try:\n",
    "        # Evaluate the model\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Get the encoded representations\n",
    "            encoded, _ = model(x_train)\n",
    "            encoded = encoded.to('cpu')\n",
    "\n",
    "            y_pred = sklearn.cluster.KMeans(n_clusters=len(set(y)), n_init=100, random_state=97).fit_predict(encoded.numpy())\n",
    "            nmi = sklearn.metrics.normalized_mutual_info_score(y, y_pred)\n",
    "            ncut = compute_ncut(nts, y_pred)\n",
    "            \n",
    "            # Print average nmi and ncut\n",
    "            print(\"[*] nmi =\", nmi)\n",
    "            print(\"[*] ncut =\", ncut)\n",
    "            \n",
    "            # If average nmi is better than the best so far, update best_nmi\n",
    "            if nmi > best_nmi:\n",
    "                best_nmi = nmi\n",
    "            \n",
    "            # If average ncut is better than the best so far, update best_ncut and its corresponding average nmi (i.e. best_ncut_nmi)\n",
    "            if ncut < best_ncut:\n",
    "                best_ncut = ncut\n",
    "                best_ncut_nmi = nmi\n",
    "    \n",
    "    except sklearn.exceptions.ConvergenceWarning:\n",
    "        print(\"[!] KMeans did not converge (not enough distinct points) --> Returning inf for ncut\")\n",
    "        ncut = float('inf')\n",
    "\n",
    "    # Return ncut as the objective to minimize\n",
    "    return ncut\n",
    "\n",
    "\n",
    "# Set global parameters\n",
    "nb_epochs_per_layer_pool = [10, 100, 500, 1000, 2500, 5000]\n",
    "nb_kmeans_tests = 100\n",
    "nb_trials = 20\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu'); print(\"[*] using device:\", device)\n",
    "x_train = torch.tensor(nts, dtype=torch.float32).to(device)\n",
    "batch_size = x_train.shape[0]\n",
    "max_time_per_layer = 3 * 60  # seconds\n",
    "loss_tolerance = 1e-4\n",
    "stab_tolerance = 5\n",
    "\n",
    "# Set globals to track best results\n",
    "best_nmi = 0.0\n",
    "best_ncut = float('inf')\n",
    "best_ncut_nmi = 0.0\n",
    "\n",
    "# Run optuna study\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(sampler=sampler, direction=\"minimize\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study.optimize(objective, n_trials=nb_trials)\n",
    "\n",
    "# Display the best results\n",
    "print(\"========================================================\")\n",
    "print(\"========================================================\")\n",
    "print(\"[*] best nmi =\", best_nmi)\n",
    "print(\"[*] best ncut =\", best_ncut)\n",
    "print(\"[*] best ncut nmi =\", best_ncut_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea0341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
